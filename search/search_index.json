{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"csc369/","title":"CSC369","text":"<p>Concept</p> <p>Role of OS</p> <ul> <li>virtualization: Present physical reource as API and virtualize different machines</li> <li>resource manager</li> </ul> <p>Demo</p> <pre><code>less cpu.c\n!setarch\nnumactl --physcpubind 1 ./cpu\n</code></pre> <pre><code>./mem 10 &amp; ./mem 200\n</code></pre> <p>From this demo, we saw the address of the two processes are the same but the actual physical address is different. That's the role of virtualization of OS</p> <ul> <li>Protection domains</li> <li>Interrupts</li> <li>Timers</li> <li>Memory Management unit</li> </ul> <p>Interrupts</p> <p>Register:</p> <ul> <li>IDTR: interrupt descriptor table</li> <li>gpRegs: general purpose register</li> </ul> <p>pmap\u547d\u4ee4: \u67e5\u770b\u5185\u5b58\u5bf9\u5e94\u7684\u5185\u5bb9</p> <p>dispatch: change state of process to \"running\"</p> <p>X86 assembly</p> instruction meaning mov (address) %eax get the memory value at <code>address</code> and copy to register <code>eax</code> mov %eax (address) get the register value at <code>eax</code> and copy to <code>address</code> add $0x1 %eax add 1(0x1) to the contents of register <code>eax</code> <p>Limited direct execution</p> <p>dual-mode operation:</p> <ul> <li>user mode</li> <li>kernel mode</li> </ul> <p>What can switch from user mode to kernel mode(two only ways):</p> <ul> <li>A hardware interrupt causes a switch to system mode</li> <li>Code executing in user mode can cause an exception, which causes the CPU to switch to system mode.</li> </ul> <p>privileged instructions: instructions that can only be executed by CPU in kernel mode, examples include:</p> <ul> <li>I/O instructions and Halt instructions</li> <li>Turn off all Interrupts</li> <li>Set the Timer</li> <li>boots up machine</li> <li>initialize trap table</li> <li>Context Switching</li> <li>Clear the Memory or Remove a process from the Memory</li> <li>Modify entries in the Device-status table</li> <li>trap/return-from-trap insruction</li> <li>hardware interrupt handler</li> <li>software exception handler</li> <li>Writing to hardware device registers(not including general used CPU registers, but some special CPU registers such as the register that stores the location of interrupt table)</li> <li>set up privilege level</li> </ul> <p>What can switch user mode to kernel mode:</p> <ul> <li>software exception(signal)</li> <li>hardware interrupt</li> <li>timer interrupt</li> <li>calling syscall</li> </ul> <p>non-privileged instructions: </p> <ul> <li>read the status of processor</li> <li> <p>read the system time</p> </li> <li> <p>generate any trap instruction</p> </li> <li>send the final printout of Printer</li> </ul> <p>trap instruction: jumps into the kernel and raises the privilege level to kernel mode</p> <p>return-from-trap instruction:  returns into the calling user program while simultaneously reducing the privilege level back to user mode  </p>"},{"location":"csc369/#1-direct-execution-protocol","title":"1. Direct Execution protocol","text":""},{"location":"csc369/#2-limited-direct-execution-protocol","title":"2. Limited Direct Execution protocol","text":"<p>init a process &amp; call systemcall:</p> <p></p> <p>timer interrupt:</p> <p></p> <p>context switch(process A to process B):</p> <p>Handle trap Call switch() routine   save kernel-regs(A) to proc-struct(A)   restore kernel-regs(B) from proc-struct(B) switch to kernel-stack(B) return-from-trap (into B)</p>"},{"location":"csc369/#3-system-call","title":"3. System call","text":"<p>Examples of system call:</p> <ul> <li>getpid</li> <li>read/write</li> <li>sbrk</li> <li>mmap: another approach for creating shared memory regions</li> <li>shmget(): System call to allocate a shared memory segment</li> <li>shmat(): map a shared memory segment to a local address</li> <li>fork</li> <li>ptrace</li> <li>syscall(syscall_no, arg1, arg2, arg3, ...)</li> <li>exit</li> </ul> <p>Flow of function call:</p> <ol> <li>Save current state of regs.</li> <li>Move args to regs</li> <li>call groundhog</li> <li>Set up stack, regs to execute function</li> <li>Set regs up for return.</li> <li>ret (retq on 64 bit)</li> <li>Move return value into \u2018result\u2019</li> <li>Restore registers</li> </ol> <p>Flow of systemCall:</p> <ol> <li>Load system call number and arguments into registers</li> <li>User process runs syscall instruction(Execute SYSENTER instruction) to trap to kernel</li> <li>Switches mode and invokes system call handler</li> <li>Lookup system call function in syscall table</li> <li>Kernel invokes corresponding function</li> <li>Kernel returns by running iret (interrupt return)</li> </ol> <p>How does system call verify parameter:</p> <ol> <li>A fixed number of arguments can be passed in registers</li> <li>Say user passes a buffer pointer, Kernel must copy data from user space into its own buffers. This is done through 2  safe functions to access user pointers: copy_from_user(), copy_to_user()</li> </ol>"},{"location":"csc369/#4-process-api","title":"4. process API","text":"<p>execute</p> <p>fork</p> <p>exit</p>"},{"location":"csc369/#5-interrupt","title":"5. Interrupt","text":"<p>process of interruption happens:</p> <ol> <li>OS fills in Interrupt Table (at boot time), sets IDTR</li> <li>CPU execution loop:</li> <li>Fetch instruction at PC</li> <li>Decode instruction</li> <li>Execute instruction</li> <li>Interrupt occurs (signal from hardware)</li> <li>CPU changes mode, disables interrupts</li> <li>Interrupted PC value is saved</li> <li>IDTR + interrupt number is used to set PC to start of interrupt handler</li> <li>Execution continues (saves additional state as first step)</li> </ol>"},{"location":"csc369/#multi-thread-system","title":"Multi-thread system","text":"<p>Textbook: Ch26, 27, 28, 29, 30, 31, 32.1-32.3, 7, 8, 9</p> <p>Slide: L3 - L9</p> <p>A multi-thread system is composed of the following:</p> <ul> <li>scheduler</li> <li>different threads</li> <li>ready queue</li> <li>wait queue(condition variable)</li> <li>lock</li> <li>zombie queue</li> </ul>"},{"location":"csc369/#1-thread-vs-process","title":"1. Thread vs process","text":"<p>Comparing with a process, a thread is much smaller.  A process can contain multiple threads</p> <p>Each process has an indendent:</p> <ul> <li>address space: code, heap, multiple stacks for different threads in this process and the stack pointer</li> <li>program counter</li> <li>OS resources: open file table</li> </ul> <p>While a thread has independent:</p> <ul> <li>stack and its stack pointer</li> <li>program counter</li> <li>registers</li> </ul>"},{"location":"csc369/#2-thread-api","title":"2. Thread API","text":"<p>The standardized C language threads programming API is called POSIX Thread. This is just an abstract interface, which is known as pthread. The implementation of this interface includes:</p> <ul> <li>kernel level thread</li> <li>requires system calls, which makes it less cheaper and faster than user level thread</li> <li>monitored and scheduled by OS</li> <li>User level thread:</li> <li>cheap and fast. Users implement user-level threads by themselves</li> <li>not scheduled by OS, which lead to poor decision</li> </ul>"},{"location":"csc369/#apis","title":"APIs:","text":"<ul> <li> <p><code>int pthread_create(pthread_t *p, const pthread_attr_t *x, void* (*fp)(void*), void* args)</code>: </p> </li> <li> <p>create a new thread that will run the function pointed by <code>fp</code> and its arguments pointed by <code>args</code>; </p> </li> <li>make <code>p</code> pointed to this new thread; </li> <li>x can be initialized through <code>pthread_attr_init()</code>, or pass<code>NULL</code>; </li> <li> <p>mark its status as ready and put itself in the ready queue</p> </li> <li> <p><code>void pthread_exit()</code>: </p> </li> <li> <p>clean the threads' stack or allocated memory in zombie queue</p> </li> <li>mark itself as exited and put itself in the zombie queue</li> <li> <p>shift to the first thread in ready queue(not yield)</p> </li> <li> <p><code>int pthread_yield(int tid)</code>:</p> </li> <li> <p><code>void pthread_kill(int tid)</code>:</p> </li> <li> <p><code>int pthread_mutex_lock(pthread_mutex_t *mutex)</code>: add lock to a thread</p> </li> <li> <p><code>int pthread_mutex_unlock(pthread_mutex_t *mutex)</code>:  unlock the lock to a thread</p> </li> <li> <p><code>int pthread_mutex_init(pthread_mutex_t *mutex, void *attr)</code>: initialize mutex object</p> </li> <li> <p><code>int pthread_mutex_destroy()</code></p> </li> <li> <p><code>int pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex)</code>: put the threads into condition variable queue to sleep</p> </li> <li> <p><code>int pthread_join(pthread_t *p, void **value_ptr)</code>: Put the current thread(the thread that calls <code>pthread_join</code>) into a wait queue till the thread <code>p</code> returns; if the function run by thread <code>p</code> returns a value, then <code>*value_ptr</code> can store the address of that return value(usually the address of the return value is in the heap allocated by <code>malloc</code>)</p> </li> <li> <p><code>int pthread_cond_signal(pthread_cond_t *cond)</code>: wake a sleeping thread waiting on the condition variable.</p> </li> </ul> <p>lifecycle of a thread/process:</p> <p></p>"},{"location":"csc369/#3-scheduler","title":"3.  scheduler","text":"<p>What is scheduler:</p> <p>Schedulers schedule which threads to run according to schedule policy. Schedule policy determines which threads to run next. Good schedulers reduce I/O and wait time. In multi-threaded system, 3 things control the switch of thread: scheduler, lock, condition variable(where wait/signal operation happens)</p> <p>Concept:</p> <ul> <li>context switch</li> <li>dispatch</li> <li>CPU burst</li> <li>I/O burst</li> <li>CPU bound</li> <li>I/O bound</li> </ul> <p>Scheduler</p> <ul> <li>type of scheduling</li> <li>Non-preemptive scheduling: once the CPU has been allocated to a thread, it keeps the CPU until it terminates or blocks</li> <li>preemptive: CPU can be taken from a running thread and allocated to another</li> <li>when to schedule</li> <li>When a thread enters Ready state<ul> <li>I/O interrupts (e.g., \u201cI/O operation completed\u201d)</li> <li>Signals (e.g., SIGCONT)</li> <li>Thread creation (or admission in batch systems)</li> </ul> </li> <li>When the running thread blocks (or exits)<ul> <li>Operating system calls (e.g., start I/O operation)</li> <li>Signals (e.g., SIGSTOP)</li> </ul> </li> <li>When the running thread voluntarily yields</li> <li> <p>At fixed intervals</p> <ul> <li>Timer interrupts</li> </ul> </li> <li> <p>scheduling policy</p> </li> <li>first-come-first-served(FCFS)</li> <li>shortest-job-first(sjf)</li> <li>round robin</li> </ul>"},{"location":"csc369/#31-how-to-evaluate-scheduling","title":"3.1 How to evaluate scheduling","text":"<ul> <li> <p>fairness: each task can be eventually picked to run and get equal share of CPU(pre-emptive)</p> </li> <li> <p>turn around time: job completes time minus job arrival time to system</p> </li> <li> <p>response time: the first time a job scheduled to execute minus job arrival time to system. In reality, </p> </li> </ul> <p>it makes a system feel responsive to interactive users (i.e., users sitting and staring at the screen, waiting for a process to finish)  </p>"},{"location":"csc369/#32-scheduling-policy","title":"3.2 Scheduling policy","text":"<p>How we program this problem:</p> <p>We are given a list of tasks : [A, B, C, ...] and each element has two fields, say <code>A.arrival</code> reprsents the arrival time of the task A and <code>A.service</code> represents the service time length of task A. Based on the scheduling policy, we want to return an ordered list of task that shows the order of the task that is scheduled. Within the ordered list, we want each task have two extra fields, say <code>A.scheduled_time</code> amd <code>A.endTime</code>. These 2 fields are 2 lists of same length, which shows at when the task is scheduled and finished running. For non-preemptive scheduling policy, these 2 lists length is 1, while for pre-emptive, the length may be more than 1.</p>"},{"location":"csc369/#321-fcfs","title":"3.2.1 FCFS","text":"<ul> <li>non-preemptive</li> <li>At any timestamp a task finishes, choose(pop) the thread at the head of the FIFO queue of ready threads; If at this timestamp, a new task arrives, we select, then put the thread at the tail of rq</li> <li>drawback: high average turn around time</li> <li>turn around time: job completes time minus job arrival time to system</li> <li>convoy effect: Suppose job A, B, C arrives in system simultaneously(i.e: \\(T_{arrival} = 0\\)) and FCFS scheduler schedules to run job A which takes 100ms, then job B that takes 10 s, then job C takes 10s, then avergage turn around time = (100-0 + 110-0 + 110-0) / 3; a number of relatively-short potential consumers of a resource get queued behind a heavyweight resource consumer  </li> </ul> <pre><code>for each time unit t:\n    if queue not empty:\n        if new task A arrive at time t:\n            add A to the tail of the queue\n        if current task curr expires and is not finished at t:\n            pop the head of the queue\n    else:\n        if new task A arrives at time t:\n            if no task is running:\n                run A\n            else:\n                put A to the tail of the queue\n        else:\n            run the current task\n</code></pre>"},{"location":"csc369/#322-sjf","title":"3.2.2 SJF","text":"<ul> <li>non-preemptive</li> <li>Know the length of each job before we run the job, we then select the job with the shortest length at each arrival time(unrealistic, similar to the Belady algorithm in 5.4 in memory management chapter)</li> <li>advantage: optimizes turn around time</li> </ul>"},{"location":"csc369/#324-round-robin","title":"3.2.4 Round Robin","text":"<p>slide: L9-Scheduling p19~32</p> <ul> <li>pre-emptive</li> <li>advantage: reduce response time</li> <li>algorithm:</li> </ul> <pre><code>for each time unit t:\n    if queue not empty:\n        if new task A arrive at time t:\n            add A to the tail of the queue\n        if current task curr expires and is not finished at t:\n            add curr to the tail of the queue\n            pop the head of the queue\n    else:\n        if new task A arrives at time t:\n            if no task is running:\n                run A\n            else:\n                put A to the tail of the queue\n        else:\n            run the current task\n</code></pre>"},{"location":"csc369/#325-multi-level-feedback-queuemlfq","title":"3.2.5 multi-level feedback queue(MLFQ)","text":"<ul> <li>Each thread has a priority</li> <li>highest priority job is selected from Ready queue</li> <li>Can be preemptive or non-preemptive</li> </ul> <p>Single-level implementation:</p> <p>Prepare 1 global ready queue</p> <p>if readyqueue is sorted:</p> <p>\u200b </p> <p>MLQ:</p> <ul> <li>Have multiple ready queues, one per priority level. Each task is only in 1 queue</li> <li>Processes are permanently assigned to a queue(this will change in feedback queue)</li> <li>Top-level scheduler picks queues in priority order</li> <li>Each queue can have its own scheduling algorithm</li> </ul> <p>Basic rules:</p> <p>Suppose A and B are processes, then:</p> <ol> <li>If Priority(A) &gt; Priority(B), A runs (B does not).</li> <li> <p>If Priority(A) = Priority(B),A and B run in round-robin fashion using the time slice (quantum length) of the given queue</p> </li> <li> <p>If pre-emptive, high priority task could preempt low priority task</p> </li> </ol> <p>MLFQ:</p> <ul> <li>Rather than giving a fixed priority to each job in MLQ, MLFQ varies the priority of a job based on its observed behavior.</li> </ul> <p>Rules of varying priority</p> <p>The allotment is the amount of time a job can spend at a given priority level before the scheduler reduces its priority. For simplicity, at first, we will assume the allotment is equal to a single time slice</p> <ol> <li>When a job enters the system, it is placed at the highest priority (the topmost queue)</li> <li>prority reduction:</li> <li>If a job uses up its allotment while running, its priority is reduced (i.e., it moves down one queue).</li> <li>If a job gives up the CPU (for example, by performing an I/O operation) before the allotment is up, it stays at the same priority level (i.e., its allotment is reset).</li> <li>After some time period S, move all the jobs in the system to the topmost queue</li> </ol> <p>Limit of MLQ:</p> <ul> <li>priority inversion:  a low priority thread holds a lock and is preempted. A high priority thread   that wants the lock cannot make progress until the low priority thread runs again. In this case, low p thread prevents high p thread from running</li> <li>solution(Priority inheritance):  Priority of task holding the mutex (semaphore) inherits the priority of a higher priority task when the higher priority task requests the semaphore.</li> <li>starvation</li> </ul>"},{"location":"csc369/#326-proportional-share-scheduling","title":"3.2.6 Proportional-Share Scheduling","text":"<p>Add a num_tickets field to PCB At scheduling time: Generate a random ticket number winner Loop over processes, keep a counter, for each process, we add the num_tickets of the process to the counter \u2022 If counter &gt; winner then pick that process</p>"},{"location":"csc369/#33-example-scheduler","title":"3.3 Example scheduler","text":""},{"location":"csc369/#331-unix-scheduler","title":"3.3.1 Unix scheduler","text":"<p>Unix scheduler uses MLFQ and RR,</p> <p>CALCULATION of priority(based on history):</p> <p></p>"},{"location":"csc369/#332-unix-scheduler24","title":"3.3.2 Unix scheduler(2.4)","text":""},{"location":"csc369/#4-synchronization-action","title":"4. synchronization action","text":"<p>synchronization</p>"},{"location":"csc369/#41-lock","title":"4.1  lock","text":"<p>Background: Race condition can happen for multi-thread program. When one thread has not finished manipulating the shared resource, another thread may emit interrupt signal and start to manipulate the shared resource. This causes race condition. Thus, we want to implement mutual exclusion. The solution to this is lock. This s the basis of <code>pthread_mutex_lock</code> and <code>pthread_mutex_unlock</code></p> <p>Concept:</p> <ul> <li> <p>race condition: When multiple concurrent threads manipulate the shared resource without synchronization, outcome depends on the order in which accesses take place. This phenomenon is called race conditions</p> </li> <li> <p>critical section: The segment of code when a set of threads access the shared variables</p> </li> <li> <p>mutual exclusion: To implement this, we need:</p> </li> <li> <p>Only one thread at a time can execute in the critical section</p> </li> <li>all other theads are forced to wait on entry(entry and exit are illustrated in below screenshot)</li> <li>When a thread leaves the entry, another can enter</li> </ul> <p></p> <ul> <li>lock: The lock is the data structure to implement mutual exclusion. It contains the status variable to indicate locked or unlocked, may contain owner thread(Thread ID), list of threads to acquire the lock.</li> </ul> <p>Evaluate of the lock implementation: To evaluate the implementation of lock, we need to assess whether the lock satisfy the following:</p> <ul> <li>Minimal requirement: does it implement mutual exclusion</li> <li>fairness:(need queue to maintain)</li> <li>When CS is free, does each thread have a fair chance to enter it</li> <li>At the end, are all threads eventually be able to execute CS</li> <li>Performance: The cost of entering and exiting CS is small</li> </ul>"},{"location":"csc369/#411-analysis-of-lock-and-bad-lock","title":"4.1.1 Analysis of lock and bad lock:","text":"<ol> <li> <p>check mutual exclusion(case 1: suppose context switch does not happen in <code>lock</code>):</p> </li> <li> <p>Suppose for a code snippet as following. Imagine thread A and thread B, first runs the <code>lock()</code> using thread A and suppose there is no context switch interruption when running <code>lock()</code> in thread A and keep track of the lock variable values</p> <pre><code>lock()\n...\n# critical section\n...\nunlock()\n</code></pre> </li> <li> <p>After keeping track of the lock variables, now suppose thread B is running <code>lock()</code> and see if the lock variable values can keep thread B not running out of <code>lock()</code></p> </li> <li> <p>Turn to thread A, run <code>unlock()</code> and keep track of lock variable values again. Switch to thread B and see if the new lock variables can let thread B run out of <code>lock()</code> and enter critical section</p> </li> <li> <p>check thread switch for each line of code in <code>lock()</code></p> </li> <li> <p>after executing the first line in thread A, divide into cases: Suppose the next line is executed in thread A or the next line is executed</p> </li> <li>For the rest of each line, do as above</li> <li>Check if there is a case with two threads not escaping the lock or two threads both escaping out from the lock and enter the CS region. If there is, the lock fails. O.W, it succeeds</li> </ol> <p>We use the above flow to analyze the implementation:</p> <p>step 1 is checked</p> <p>we then consider the case when context switch happens in lock. The first line is definitely executed by thread A. Suppose context switch happens and thread B starts to execute the first line(comparison) and execute the second line(change the flag). Now thread B escapes out of lock and context switch to thread A. Since flag is set by thread B and it stucks in lock. In this way, we initially want to lock for thread A, but thread B gets the lock. The implementation fails</p> <p>Some bad examples of lock implementation:</p> <ul> <li>ex1:</li> </ul> <pre><code>typedef struct __lock_t { int flag; } lock_t;\n\nvoid init(lock_t *mutex) {\nmutex-&gt;flag = 0; // 0 -&gt; lock is available, 1 -&gt; held\n}\n\nvoid lock(lock_t *mutex) {\nwhile (mutex-&gt;flag == 1) // TEST the flag\n; // spin-wait (do nothing)\nmutex-&gt;flag = 1; // now SET it!\n}\n\nvoid unlock(lock_t *mutex) {\nmutex-&gt;flag = 0;\n}\n</code></pre> <p>(explanation: suppose thread A and thread B calls lock before manipulate shared variable.  The following order will have both threads entering lock: </p> <p>A executes 6 -&gt; A is going to execute line 8 -&gt; context switch -&gt; B executes line 6 -&gt; B executes line 8 and comes out of the lock -&gt;B executes code in CS -&gt; context switch -&gt; A executes line 8 and comes out of the lock())</p>"},{"location":"csc369/#412-good-lock-implementation","title":"4.1.2 good Lock implementation","text":""},{"location":"csc369/#4121-interrupt","title":"4.1.2.1 interrupt","text":"<pre><code>lock(){\n    disable_interrupts()\n}\n\nunlock(){\n    enable_interrupts()\n}\n</code></pre> <p>Evaluation:</p> <ul> <li>mutual exlusive: It achieves mutual exclusive on uniprocessor sytem since it won't let other threads send interrupt signal. However, it does not achieve mutual exclusive on multi-processor system</li> <li>not very cheap and slow</li> <li>other(security): </li> <li>It performs a privileged operation(turing interrupt off). The facility is unsafe when it's under abuse and the interrupt is off</li> <li>lead to serious system problems when interrupts are lost for long time.(ex:  CPU may miss a read request due to not being able to receive signal)</li> </ul>"},{"location":"csc369/#4122-mannually-implement","title":"4.1.2.2  mannually implement","text":"<p>There are variations of this lock implementation. We give one implementation example</p> <p>Peterson's algorithm(for only 2 threads):</p> <p>Peterson's algorithm(for n threads)</p>"},{"location":"csc369/#4123-spin-lock","title":"4.1.2.3 spin lock","text":"<p>The characteristic of spin lock is that there is a while loop inside the lock function that causes the second thread into an infinite loop to block thread 2 when thread 1 is executing the critical section. For spin lock, we have 4 implementations:</p> <ul> <li>test-and-set</li> <li>compared-and-swap</li> <li>loadlinked and StoreConditional(not going to cover)</li> <li>fetch-and-add(not going to cover)</li> </ul> <p>test-and-set:</p> <p>The spin lock is based on the first bad lock example we see in 4.1.1. We modify it and add TestAndSet instruction because context switch can happen between checking the unlocked lock and lock it. We have an atomic hardware instruction called test-and-set:</p> <pre><code>int TestAndSet(int *old_ptr, int new){\n    int old = *old_ptr;\n    *old_ptr = new;\n    return old;\n}\n</code></pre> <p>How we use test-and-set in <code>lock</code> :</p> <pre><code>typedef struct __lock_t {int flag;} lock_t;\n\nvoid init(lock_t *lock){\n    // 0 indicates that lock is available, 1 that is held\n    lock-&gt;flag = 0;\n}\n\nvoid lock(lock_t *lock){\n    while (TestAndSet(&amp;lock -&gt; flag, 1) == 1)\n        ; // spin-wait (do nothing)\n}\n\nvoid unlock(lock_t *lock){\n    unlock -&gt; flag =0;\n}\n</code></pre> <p>compare-and-swap:</p> <p>atomic hardware instruction  compare-and-swap:</p> <pre><code>int CompareAndSwap(int *ptr, int expected, int new){\n    int actual = *ptr;\n    if(actual == expected)\n        *ptr = new;\n    return actual;\n}\n</code></pre> <p>How we use test-and-set in <code>lock</code>:</p> <pre><code>typedef struct __lock_t {int flag;} lock_t;\n\nvoid init(lock_t *lock){\n    // 0 indicates that lock is available, 1 that is held\n    lock-&gt;flag = 0;\n}\n\nvoid lock(lock_t *lock){\n    while (CompareAndSwap(&amp;lock -&gt; flag, 0, 1) == 1)\n        ; // spin-wait (do nothing)\n}\n\nvoid unlock(lock_t *lock){\n    unlock -&gt; flag =0;\n}\n</code></pre>"},{"location":"csc369/#4124-sleep-lock","title":"4.1.2.4 sleep lock","text":"<p>The sleep lock makes the thread yield, which is similar to thread_wait that also puts thread into sleep(change the thread state to blocked and put in block queue). The sleep lock requires the function of <code>park</code> and <code>unpark</code>:</p> <ul> <li><code>park()</code>: shifts the thread that calls <code>park</code> to another thread</li> <li><code>setpark()</code>: It indicates it is about to park</li> <li><code>unpark(ThreadID)</code>: shift to thread denoted by ThreadID and wake the thread</li> </ul> <p>One implementation:</p> <pre><code>typedef struct __lock_t {\n    int flag;\n    int guard;\n    queue_t *q;\n} lock_t;\n\nvoid lock_init(lock_t *m) {\n    m -&gt; flag = 0;\n    m -&gt; guard = 0;\n    queue_init(m-&gt;q);\n}\n\nvoid lock(lock_t *m) {\n    while(TestAndSet(&amp;m-&gt;guard, 1) == 1)\n        ;\n    if(m-&gt;flag == 0){\n        m-&gt;flag = 1;\n        m-&gt;guard = 0;\n    } else{\n        queue_add(m-&gt;q, gettid());\n        setPark();\n        m-&gt;guard = 0;\n        park();\n    }\n}\n\nvoid unlock(lock_t *m) {\n    while(TestAndSet(&amp;m-&gt;guard, 1) == 1)\n        ;\n    if(queue_empty(m-&gt;q))\n        m-&gt;flag = 0;\n    else\n        unpark(queue_remove(m-&gt;q));\n    m-&gt;guard = 0;\n}\n</code></pre>"},{"location":"csc369/#4125-semaphore","title":"4.1.2.5 Semaphore","text":"<p>Semaphore can be found in 4.3. Semaphore can be used to implement sleep lock by initializing the counter to be 1:</p> <pre><code>#include &lt;semaphore.h&gt;\ntypedef struct __lock_t {sem_t m} lock_t;\nsem_t m;\nsem_init(&amp;m, 0, 1);\n// 0 tells al threads to share m; initializes the counter of m to be 1 and cv to be an empty queue\nlock_t lock;\nlock.m = m;\n\nvoid lock(lock_t *lock){\n    sem_wait(&amp;lock-&gt;m);\n    // decrement the counter of semaphore s by 1\n    // wait in semaphore's wait queue if (s-&gt;counter &lt; 0)\n}\n\nvoid unlock(lock_t *lock){\n    sem_post(&amp;lock-&gt;m);\n    // increment the counter of semaphore s by 1\n    // if there is one or more thread in wait queue, wake the first thread and the first thread will execute code in CS no matter the m.counter is less than 0(when multiple threads call sem_wait and m.counter becomes a very small negative value)\n}\n</code></pre>"},{"location":"csc369/#42-condition-variable","title":"4.2 condition variable","text":"<p>We want to make sure some threads are executed before certain threads. The solution to this is condition variable. This is the basis of methods: <code>thread_join</code>, <code>thread_exit</code>, <code>thread_wait</code>, <code>thread_signal</code>, <code>thread_broadcast</code></p> <p>what is condition variable:</p> <p>It's a queue plus a lock that threads put themselves on when some state of execution (i.e some conditions) is not desired. Some other thread, when it changes said state, can then wake one (or more) of those waiting threads and thus allow them to continue (by signaling on the condition)  </p> <p>Implementation:</p> <pre><code>pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER; // mutex also has a queue\npthread_cond_t c = PTHREAD_COND_INITIALIZER;  // c is basically a queue\nint ready = 0;\n\n// when multiple threads wait for thread A, other threads call this function\nvoid pthread_join(){\n    pthread_mutex_lock(&amp;m);\n    while(ready == 0)\n        pthread_cond_wait(&amp;c, &amp;m);\n\n    pthread_mutex_unlock(&amp;m);\n}\n\n// when the thread A has terminated, thread A calls this function\nvoid thread_release() {\n    pthread_mutex_lock(&amp;m);\n    dosomething();\n    ready = 1;    \n    // dosomething change he condition in pthread_join\n    pthread_cond_signal(&amp;c);\n    pthread_mutex_unlock(&amp;m);\n}\n\nvoid pthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m){\n    // this makes the thread sleep or wake up from this call when signalled\n    // step1: release the lock\n    pthread_mutex_unlock(m);\n    int shifted = 0;\n    getContext(&amp;curr_thread.context);\n    // step 2: determine if we execution is to put the thread to sleep or this thread \n    //         is signaled by other and wakes up\n    if(shifted == 0){\n        // this case means we want to put this thread to sleep      \n        shifted = 1;\n        // step 3 &amp; 4: put curr_thread in c and shift to an thread in ready_queue \n        // atomically; make the above set of shifted atomical as well; \n        //atomicity can be done by disabling interrupt\n    } else{\n        // this case means this thread is signaled and wake up\n        pthread_mutex_lock(m);\n    }\n}\n\nvoid pthread_cond_signal(pthread_cond_t *c){\n    // step1: pop one blocked thread from conditional variable `c`\n    // step2: change the blocked thread status to ready\n    // step 3: put the thread in the ready queue(don't shift the current thread context)\n}\n\nvoid pthread_cond_broadcast(pthread_cond_t *c){\n    // step1: pop all blocked thread from conditional variable `c`\n    // step2: change all blocked thread status to ready\n    // step 3: put all threads in the ready queue(don't shift the current thread context)\n}\n</code></pre>"},{"location":"csc369/#43-semaphore","title":"4.3 Semaphore","text":"<p>What is Semaphore:</p> <p>Semaphore is a data structure that can be used to implement the function of both lock and condition variable. (Actually, the implementation of semaphore is also compose of lock and condition variable). The data structure includes:</p> <ul> <li> <p>internal property</p> </li> <li> <p>an integer counter variable modified by its 2 atomic operations</p> </li> <li> <p>a queue of waiting thread</p> </li> <li> <p>Atomic operations:</p> </li> <li> <p><code>sem_wait</code>: </p> </li> <li> <p><code>sem_post</code>: </p> <pre><code>typedef struct sem_t {\n    int counter;\n    Queue cv;\n}sem_t;\n                                                                                                                                                \nsem_t m;\nsem_init(&amp;m, 0, x);  // 0 tells al threads to share m; initializes the counter of m to be x and cv to be an empty queue\n                                                                                                                                                \nint sem_wait(sem_t *s){\n    // decrement the counter of semaphore s by 1\n    // wait if (s-&gt;counter &lt; 0)\n}\n                                                                                                                                                \nint sem_post(sem_t *s){\n    // increment the counter of semaphore s by 1\n    // if there is one or more thread in wait queue, wake the first thread\n}\n</code></pre> </li> </ul> <p>how to use semaphore to implement lock:</p> <p>we initialize the counter to be 1:</p> <pre><code>#include &lt;semaphore.h&gt;\ntypedef struct __lock_t {sem_t m} lock_t;\nsem_t m;\nsem_init(&amp;m, 0, 1);\nlock_t lock;\nlock.m = m;\n\nvoid lock(lock_t *lock){\n    sem_wait(&amp;lock-&gt;m);\n}\n\nvoid unlock(lock_t *lock){\n    sem_post(&amp;lock-&gt;m);\n}\n</code></pre> <p>how to use semaphore to implement condition variable:</p> <pre><code>#include &lt;semaphore.h&gt;\ntypedef struct __lock_t {sem_t m} lock_t;\nint N; // N &gt; 1, which represents Max number of threads that can pass the semaphore\nsem_t m;\nsem_init(&amp;m, 0, N);\nlock_t lock;\nlock.m = m;\n\nvoid lock(lock_t *lock){\n    sem_wait(&amp;lock-&gt;m);\n}\n\nvoid unlock(lock_t *lock){\n    sem_post(&amp;lock-&gt;m);\n}\n</code></pre>"},{"location":"csc369/#44-bounded-buffer-problem","title":"4.4 Bounded buffer problem","text":""},{"location":"csc369/#441-use-pcond_wait","title":"4.4.1 use pcond_wait","text":"<p>broken soln 1(one condition variable and use if check instead of while check to read 1 byte at a time):</p> <p>problem: (if there are more than 2 consumers, this fails. Suppose 2 consumers and 1 producer, consider the case consumer1 -&gt; producer -&gt; consumer2 -&gt; consumer1, even the buf is empty, the consumer2 still gets garbage data from buffer and change buffer size, this works when there is only one producer and one consumer)</p> <pre><code>#define MAX 10\ntypedef struct buf_s {\n    int data[MAX]; /* buffer storage */\n    int in_pos; /* producer inserts here */\n    int out_pos; /* consumer removes from here */\n    int num_elements; /* # items in buffer */\n    pthread_mutex_t buflock; /* access to buffer */\n    pthread_cond_t one_cond; /* for producers to wait*/\n} buf_t;\nbuf_t buffer;\n\nint producer(buf_t *b, int value){\n    pthread_mutex_lock(&amp;b-&gt;buflock);\n    if(b-&gt;num_elements == MAX) {\n        /* buffer b is full, wait */\n        pthread_cond_wait(&amp;b-&gt;one_cond, &amp;b-&gt;buflock);\n    }\n    b-&gt;data[b-&gt;in_pos] = value;\n    b-&gt;in_pos = (b-&gt;in_pos + 1) % MAX;\n    b-&gt;num_elements++;\n    /* buffer b is not empty, signal consumer */\n    pthread_cond_signal(&amp;b-&gt;one_cond);\n    pthread_mutex_unlock(&amp;b-&gt;buflock);\n}\n\nint consumer(buf_t * b){\n    int val;\n    pthread_mutex_lock(&amp;b-&gt;buflock);\n    if (b-&gt;num_elements == 0) {\n        /* buffer b is empty, wait */\n        pthread_cond_wait(&amp;b-&gt;one_cond, &amp;b-&gt;buflock);\n    }\n    val = b-&gt;data[b-&gt;out_pos];\n    b-&gt;out_pos = (b-&gt;out_pos + 1) % MAX;\n    b-&gt;num_elements--;\n    pthread_cond_signal(&amp;b-&gt;one_cond);\n    pthread_mutex_unlock(&amp;b-&gt;buflock);\n    return val;\n}\n</code></pre> <p>broken soln 1(one condition variable with while loop to read one byte at a time):</p> <p>(consumer: if the buf length is more than 1, suppose there are 2 consumers and 1 producer, the sequence is c1 -&gt; c2 -&gt; p -&gt; p-&gt; c1 -&gt; c1 -&gt; c2, then at the end, this will reesult a deadlock that both consumers and producer sleep at the end,  at the 2<sup>nd</sup> p, producer sleeps, then first c1suppose consume all data, wakesup c2, annd sleep at the scond c1, finally c2 goes to sleep; however, this soln still work when there is only 1 consumer and 1 producer because wakeup producer when there is last one awake consumer is guranteed )</p> <pre><code>#define MAX 10\ntypedef struct buf_s {\n    int data[MAX]; /* buffer storage */\n    int in_pos; /* producer inserts here */\n    int out_pos; /* consumer removes from here */\n    int num_elements; /* # items in buffer */\n    pthread_mutex_t buflock; /* access to buffer */\n    pthread_cond_t one_cond; /* for producers to wait*/\n} buf_t;\nbuf_t buffer;\n\nint producer(buf_t *b, int value){\n    pthread_mutex_lock(&amp;b-&gt;buflock);\n    while(b-&gt;num_elements == MAX) {\n        /* buffer b is full, wait */\n        pthread_cond_wait(&amp;b-&gt;one_cond, &amp;b-&gt;buflock);\n    }\n    b-&gt;data[b-&gt;in_pos] = value;\n    b-&gt;in_pos = (b-&gt;in_pos + 1) % MAX;\n    b-&gt;num_elements++;\n    /* buffer b is not empty, signal consumer */\n    pthread_cond_signal(&amp;b-&gt;one_cond);\n    pthread_mutex_unlock(&amp;b-&gt;buflock);\n}\n\nint consumer(buf_t * b){\n    int val;\n    pthread_mutex_lock(&amp;b-&gt;buflock);\n    while (b-&gt;num_elements == 0) {\n        /* buffer b is empty, wait */\n        pthread_cond_wait(&amp;b-&gt;one_cond, &amp;b-&gt;buflock);\n    }\n    val = b-&gt;data[b-&gt;out_pos];\n    b-&gt;out_pos = (b-&gt;out_pos + 1) % MAX;\n    b-&gt;num_elements--;\n    pthread_cond_signal(&amp;b-&gt;one_cond);\n    pthread_mutex_unlock(&amp;b-&gt;buflock);\n    return val;\n}\n</code></pre> <p>Final soln:</p> <pre><code>#define MAX 10\ntypedef struct buf_s {\n    int data[MAX]; /* buffer storage */\n    int in_pos; /* producer inserts here */\n    int out_pos; /* consumer removes from here */\n    int num_elements; /* # items in buffer */\n    pthread_mutex_t buflock; /* access to buffer */\n    pthread_cond_t not_full; /* for producers to wait*/\n    pthread_cond_t not_empty;\n} buf_t;\nbuf_t buffer;\n\nint producer(buf_t *b, int value){\n    pthread_mutex_lock(&amp;b-&gt;buflock);\n    while(b-&gt;num_elements == MAX) {\n        /* buffer b is full, wait */\n        pthread_cond_wait(&amp;b-&gt;not_full, &amp;b-&gt;buflock);\n    }\n    b-&gt;data[b-&gt;in_pos] = value;\n    b-&gt;in_pos = (b-&gt;in_pos + 1) % MAX;\n    b-&gt;num_elements++;\n    /* buffer b is not empty, signal consumer */\n    pthread_cond_signal(&amp;b-&gt;not_empty);\n    pthread_mutex_unlock(&amp;b-&gt;buflock);\n}\n\nint consumer(buf_t * b){\n    int val;\n    pthread_mutex_lock(&amp;b-&gt;buflock);\n    while (b-&gt;num_elements == 0) {\n        /* buffer b is empty, wait */\n        pthread_cond_wait(&amp;b-&gt;not_empty, &amp;b-&gt;buflock);\n    }\n    val = b-&gt;data[b-&gt;out_pos];\n    b-&gt;out_pos = (b-&gt;out_pos + 1) % MAX;\n    b-&gt;num_elements--;\n    pthread_cond_signal(&amp;b-&gt;not_full);\n    pthread_mutex_unlock(&amp;b-&gt;buflock);\n    return val;\n}\n</code></pre>"},{"location":"csc369/#442-use-semaphore","title":"4.4.2 Use semaphore","text":"<p>Soln 1:(no binary semaphore as lock)</p> <p>(problem: there is no mutual exclusion, can only work if the buffer length is 1 with 1 or more producer and consumer)</p> <pre><code>#define N 1\nSem empty = sem_init(0);\nSem full = sem_init(N);\nint buffer[MAX];\n\nvoid producer(){\n    sem_wait(full);\n    add_to_buffer(buffer);\n    sem_signal(empty);\n}\n\nvoid consumer(){\n    sem_wait(empty);\n    remove_from_buffer(buffer);\n    sem_signal(full);\n}\n</code></pre> <p>Soln 2: (add deadlock)</p> <p>(problem: the mutex is acquired before sem_wait for full or empty, in this case, if the buffer is empty, consumer first acquires lock and waits for producer to sem_post on empty, producer cannot acquire lock from consumer)</p> <pre><code>#define N 10\nSem empty = sem_init(0);\nSem full = sem_init(N);\nSem mutex = sem_init(1);\nint buffer[MAX];\n\nvoid producer(){\n    sem_wait(mutex);\n    sem_wait(full);\n    add_to_buffer(buffer);\n    sem_signal(empty);\n    sem_post(mutex);\n}\n\nvoid consumer(){\n    sem_wait(mutex);\n    sem_wait(empty);\n    remove_from_buffer(buffer);\n    sem_signal(full);\n    sem_post(mutex);\n}\n</code></pre> <p>Soln3(working soln):</p> <pre><code>#define N 10\nSem empty = sem_init(0);\nSem full = sem_init(N);\nSem mutex = sem_init(1);\nint buffer[MAX];\n\nvoid producer(){\n    sem_wait(full);\n    sem_wait(mutex);\n    add_to_buffer(buffer);\n    sem_post(mutex);\n    sem_signal(empty);\n}\n\nvoid consumer(){\n    sem_wait(empty);\n    sem_wait(mutex);\n    remove_from_buffer(buffer);\n    sem_post(mutex);\n    sem_signal(full);\n}\n</code></pre>"},{"location":"csc369/#5-concurrency-problem","title":"5. Concurrency problem","text":""},{"location":"csc369/#51-all-threads-go-to-sleep","title":"5.1 All threads go to sleep","text":""},{"location":"csc369/#52-atomicity-violation","title":"5.2 atomicity violation","text":"<p>What is atomicity violation:</p> <p>The formal defn is the desired serializability among multiple memory accesses is violated. (i.e: a code region is intended to be atomic, but the atomicity is not enforced during execution). The solution to this is often add lock to wrap the critical region.</p> <p>Serializability:</p> <p>Guarantees outcome of concurrent operations will be equivalent to some sequential execution of the same operations</p>"},{"location":"csc369/#53-order-violation-bugs","title":"5.3 order violation bugs","text":"<p>What is order violation:</p> <p>The desired order between two (groups of) memory accesses is flipped (i.e., A should always be executed before B, but the order is not enforced during execution) . The solution to this is often deadlock</p>"},{"location":"csc369/#54-deadlock","title":"5.4 deadlock","text":"<p>Finding deadlock in the code:</p> <ul> <li>deadlock example1:</li> </ul> <p>first executes consumer in thread A, then executes producer in thread B leads to deadlock</p> <p>(Notes: even only one lock can lead to deadlock)</p> <pre><code>sem_t empty;\nsem_t full;\nsem_t mutex;\n\nsem_init(&amp;empty, 0, MAX); // MAX buffers are empty to begin with...\nsem_init(&amp;full, 0, 0); // ... and 0 are full\nsem_init(&amp;mutex, 0, 1);\n\nvoid *producer(void *arg){\n    sem_wait(&amp;mutex);\n    sem_wait(&amp;empty);\n    put(i);  //write to buffer\n    sem_post(&amp;full);\n    sem_post(&amp;mutex);\n}\n\nvoid *consumer(void *arg){\n    sem_wait(&amp;mutex);\n    sem_wait(&amp;full);\n    int tmp = get();\n    sem_post(&amp;empty);\n    sem_post(&amp;mutex);\n}\n</code></pre> <ul> <li>deadlock example 2:</li> </ul> <pre><code>void do_something(mutex_t *m1, mutex_t * m2){\n    lock(m1);\n      ...\n    lock(m2);\n}\n// thread A: do_something(l1, l2);\n\n// thread B: do_something(l2, l1)\n</code></pre> <p>How to prevent deadlock:</p> <p>Since the condition for deadlock to happen is due to:</p> <ul> <li>circular wait: a closed chain of processes exists  such that each process holds at least one   resource needed by the next process in the chain</li> <li>hold-and-wait: A process may hold allocated resources while awaiting assignment of others</li> <li>no-preemption: No resource can be forcibly removed from a process</li> <li>mutual exclusion: Only one process may use a resource at a time</li> </ul> <p>If we can break any of these conditions, we can avoid deadlock</p> <ul> <li>breaking circular wait:</li> <li>total ordering: To break circular wait, suppose there are multiple locks in the system, we can strictly order the sequence of the each lock being locked by each thread. </li> <li> <p>partial ordering:</p> </li> <li> <p>breakin hold-and-wait:</p> </li> <li> <p>we acquire all locks atomically at very start before other threads compete the lock</p> </li> <li> <p>ex: Suppose we have locks <code>L1</code> and <code>L2</code> in the system, we can have a global lock <code>prevention</code> to ensure all locks are held atomically as below. In this way, when other threads try to hold <code>L1</code> and <code>L2</code> in different order, it will locked out by <code>prevention</code></p> </li> <li> <p>Use <code>trylock()</code> to acquire the second lock instead of calling <code>lock()</code></p> <p>trylock implementation:</p> <pre><code>int trylock(L){\n    //grab the lock (if it is available) or return -1 indicating that the\n   //lock is held right now\n}\n\ntop:\n   lock(L1);\n   if(trylock(L2) == -1){\n        unlock(L1);\n        goto top;\n    }\n</code></pre> </li> <li> <p>breaking no-preemption:</p> </li> </ul> <p>Not feasible</p> <ul> <li>breaking mutual exclusion:</li> </ul> <p>Use CAS to replace lock</p>"},{"location":"csc369/#6-summary-problem","title":"6. Summary problem","text":"<ul> <li>look at the code without using lock that leads to race condition &amp; think of different situations from the race condition</li> <li>Determine if the lock achieves the goal</li> <li>give u a lock that fails</li> <li>give u a lock that succeeds</li> </ul>"},{"location":"csc369/#memory-management","title":"Memory Management","text":"<p>https://student.cs.uwaterloo.ca/~cs350/F06/slides/cs350_E</p>"},{"location":"csc369/#1-overview","title":"1.  Overview:","text":"<p>Virtual address(VA), Physical address(PA) vs PTE:</p> <ul> <li>VA: it's the address of the variables inside the program, not the real address in hardware</li> <li>Converting to binary string, the leftmost few bits are VPN and the rightmost few bits are offset</li> <li>Converting to a decimal number say a, VA = a represents the a<sup>th</sup> byte in the memory</li> <li>PA: it's real address of variables in hardware</li> <li>Convering to binary string, the leftmost few bits are PFN and the rightmost few bits are offset</li> <li>PTE: the entry in the page table. It contains the PFN. The OS uses VPN to index the position of PTE in the table and translate the VPN to PFN using the PFN at the PTE</li> </ul> <p>Basic information of memory system:</p> <ul> <li>the number of bits in the address space</li> <li>the size of the page</li> <li>the size of PTE</li> </ul> <p>address binding:</p> <p>Binding (or connecting) variable names to physical locations</p> <p>When are addresses bounded:</p> Compile time Load time Execution - Relocation impossible cannot relocate after loading into memory can - Cannot run more than one instance can can require relocation register(base &amp; limit) <p>partition</p> <ul> <li>fixed: Divide memory into regions with fixed boundaries(can be equal or unequal)</li> <li>dynamic: Partitions vary in length and number over time</li> </ul> dynamic fixed internal fragmentation no yes external fragmentation yes(require relocatable process) no number of partition determine processes <p>What can be infered from basic info:</p> <p>Given:</p> <ul> <li>the 32-bit address space   (1)</li> <li>page size: 4096 bytes         (2)</li> <li>PTE size: 4 bytes                  (3)</li> </ul> <p>We know:</p> <ul> <li>(1): </li> <li>the virtual address space is 32 bits, thus, (# of bits in virtual address) = 32</li> <li>the size of the RAM = 2<sup>32</sup> bytes = 4 GB</li> <li> <p>number of virtual pages = 2<sup>32</sup> / 4096 = 2<sup>20</sup>, thus, VPN bits number is 20. The VPN can be used as linear or multi-level tables</p> </li> <li> <p>(2):</p> </li> <li>the offset bits \\(\\(log_2(4096) = 12\\)\\)</li> <li>(3):</li> <li>the number of bits in each PTE: 4 * 8 = 32 bits</li> <li>number of PTEs in a single page(important to determine the levels in multi-level table): 4096 bytes / 4 bytes = 1024, thus, number of bits in each level is 10</li> </ul>"},{"location":"csc369/#2-complete-memory-access-flow","title":"2. Complete memory access flow","text":"<p>memory access flow:</p> <ul> <li>simple page table:</li> </ul> <pre><code>VPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT\n(Success, TlbEntry) = TLB_Lookup(VPN)\nif (Success == True) // TLB Hit\n    if (CanAccess(TlbEntry.ProtectBits) == True)\n        Offset = VirtualAddress &amp; OFFSET_MASK\n        PhysAddr = (TlbEntry.PFN &lt;&lt; SHIFT) | Offset\n        Register = AccessMemory(PhysAddr)\n    else\n        RaiseException(PROTECTION_FAULT)\nelse // TLB Miss\n    PTEAddr = PTBR + (VPN * sizeof(PTE))\n    PTE = AccessMemory(PTEAddr)\n    if (PTE.Valid == False)\n        RaiseException(SEGMENTATION_FAULT)\n    else\n        if (CanAccess(PTE.ProtectBits) == False)\n            RaiseException(PROTECTION_FAULT)\n        else if (PTE.Present == True)\n        // assuming hardware-managed TLB\n            TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)\n            RetryInstruction()\n        // if we adopt software-managed TLB, the line would be following:\n        // RaiseException(TLB_MISS)\n        else if (PTE.Present == False)\n            RaiseException(PAGE_FAULT)\n</code></pre> <ul> <li>double page table:</li> </ul> <pre><code>  VPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT\n  (Success, TlbEntry) = TLB_Lookup(VPN)\n  if (Success == True) // TLB Hit\n    if (CanAccess(TlbEntry.ProtectBits) == True)\n        Offset = VirtualAddress &amp; OFFSET_MASK\n        PhysAddr = (TlbEntry.PFN &lt;&lt; SHIFT) | Offset\n        Register = AccessMemory(PhysAddr)\n    else\n        RaiseException(PROTECTION_FAULT)\n  else // TLB Miss\n    // before diving into page table\n    //look at the list of VMAs in the addressSpace\n    //for each VMA in addressSpace:\n    //  if VirtualAddress in VMA:\n    //      allocate memory for that VirualAdress\n    // Go to Page table: first, get page directory entry\n    PDIndex = (VPN &amp; PD_MASK) &gt;&gt; PD_SHIFT\n    PDEAddr = PDBR + (PDIndex * sizeof(PDE))\n    PDE = AccessMemory(PDEAddr)\n    if (PDE.Valid == False)\n        RaiseException(SEGMENTATION_FAULT)\n    else\n    // PDE is valid: now fetch PTE from page table\n        PTIndex = (VPN &amp; PT_MASK) &gt;&gt; PT_SHIFT\n        PTEAddr = (PDE.PFN &lt;&lt; SHIFT) + (PTIndex * sizeof(PTE))\n        PTE = AccessMemory(PTEAddr)\n        if (PTE.Valid == False)\n            RaiseException(SEGMENTATION_FAULT)\n        else if (CanAccess(PTE.ProtectBits) == False)\n            RaiseException(PROTECTION_FAULT)\n        else\n            TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits)\n            RetryInstruction()\n</code></pre> <p>Page fault control flow:</p> <pre><code>PFN = FindFreePhysicalPage()\nif (PFN == -1) // no free page found\n    PFN = EvictPage() // run replacement algorithm\nDiskRead(PTE.DiskAddr, pfn) // sleep (waiting for I/O)\nPTE.present = True // update page table with present\nPTE.PFN = PFN // bit and translation (PFN)\nRetryInstruction() // retry instruction\n</code></pre> <ol> <li>check if the virtual address is valid in VMAs</li> <li>Check TLB for minor page fault</li> </ol> <p>process of page eviction:</p> <pre><code>if PFN.dirty = 1:\n    // writing the dirty bit to the disk is called cleaning step\n    // the cleaning step may be performed at other times\n    // or it can be performed when evicting this page as shown in the pseudocode\n    DiskWrite(PTE.DiskAddrm PFN)\nPFN.Valid = False\n</code></pre> <p>when does page eviction happens(page buffering):</p> <ol> <li>The OS has some kind of value called high watermark(HW) and low watermark(LW) and keeps a list of free pages</li> <li>periodically, when OS notices there are fewer than LW free pages, the background thread, called page daemon, evicts pages till there are HW free pages</li> <li>On page fault, grab a frame from the free list</li> </ol> <p>Page fault:</p> <p>OS tries to access pages that are not in physical memory but are written to the disk</p> <p>TLB</p> <p>Caches for quick mapping from virual page # to physical page # in each process. There are hardware-loaded TLB and software-loaded TLB</p> <p>TLB miss or TLB fault</p>"},{"location":"csc369/#3-paging-vs-partition","title":"3 Paging vs Partition","text":"<p>If the OS uses paging to manage memory, then the memory is composed of multiple pages(usually 4096 bytes).  For each process, some pages will be allocated for that process. Among these pages, OS will allocate some for page tables for address translation for each process.  Paging uses TLB to accelerate performance. With paging, we can write pages to disk through page placement policy. Compared with partition, paging is the solution for the modern operating system.</p> <p>Important topics within each paging include:</p> <ul> <li>different implementations of page tables:</li> <li>Simple linear page table</li> <li>dynamically extend page table</li> <li>page table per segment</li> <li>multi-level page table</li> <li>hashed page tables</li> <li> <p>Inverted page tables</p> </li> <li> <p>TLB</p> </li> <li>page placement policies.</li> </ul> <p>Fragmentation problem:</p> <ul> <li>internal fragmentation: The allocated continuous memory area for a process is way larger than the amount of area the process really needs. This causes waste of memory area</li> <li>external fragmentation: The continuous memory areas in the memory are quite small because the memory areas for other processes scatter. This leads to the consequence that when we try to allocate memory for a new process, there is no continuous memory area that is large enough for that new process.</li> </ul> <p>Partition solution suffers really from either internal or external fragmentation, but paging solution reduces those problems(not entirely eliminate). Because paging allows process to have memory area that does not have to be continuous. </p> <p>Paging arranges memory into small units smaller than partition. Paging solution has two components: Page size and page table. The implementation of paging varies according to the difference of page tables.</p> <p>The following will focus on implementations of paging</p>"},{"location":"csc369/#31-implementations-of-page-table","title":"3.1 implementations of page table","text":"<p>The implementations of page tables include:</p> <ul> <li>Simple linear page table</li> <li>dynamically extend page table</li> <li>page table per segment</li> <li>multi-level page table</li> <li>hashed page tables</li> <li>Inverted page tables</li> </ul> <p>Page tables contain important information in its special bits, such as valid bits, present bits, dirty bits and so on. The implementations of page tables vary in its PFN bits.</p> <p>After knowing the individual implementation, we also need to understand:</p> <ul> <li>How does the OS locate page table in memory</li> <li>how to do address translation through page table(reason for needing page table)</li> </ul> <p>How does OS find page table</p> <ul> <li>Simple linear page:</li> <li>The OS stores the physical address of the starting location of the paging table in page-table base register</li> <li>Then the OS calculates number of page tables in memory by pulling out the left few bits of FullVirtualAddress (i.e: NumberOfPTE = (FullVirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT)</li> <li>The OS calculates the end address of page table(i.e: EndAddress = PageTableBaseRegister + (NumberOfPTE * sizeof(PTE)))</li> </ul> <p>how to do address translation through page table</p> <p></p> <ul> <li>Simple linear page:</li> <li>Split the virtual address binary into 2 parts: the leftmost few bits are the virtual page number(VPN), the rightmost few bits are the offset bits</li> <li>Convert the leftmost few bits into number and index on paging table to find the physical frame number(PFN) in the paging table entries</li> <li>Replace the VPN binary string with the PFN binary string in the virtual address to get physical address</li> </ul>"},{"location":"csc369/#311-simple-linear-page","title":"3.1.1 Simple linear page","text":"<p>Suppose the memory is a 32-bit address space(the size of space would be 2<sup>32</sup> bytes = 4 GB and the length of physical address is 32 bits for each i<sup>th</sup> byte) and the size of each page is 4kB(2<sup>12</sup> bytes = 4096 bytes),</p> <p></p> <p>Then the OS allocates some memory for the simple linear page table in that process and the simple linear page table will be an array of entries with each entry 4 bytes(32 bits):</p> <ul> <li>The leftmost 20 bits are for PFN. (because there are 2<sup>32</sup> / 2 <sup>12</sup> = 2<sup>20</sup> pages). If the VPN is i in decimal digits, then the PFN for the VPN is the i<sup>th</sup> entry is page table. The address of the page is at the (PFN &gt;&gt; 12)<sup>th</sup> byte. Also, the leftmost 20 bits determine the length of page tables, where the length is 2<sup>20</sup> bytes. If the page size is larger, then the number of bits for PFN is smaller and page table size dereases</li> <li>1 bit for validity: whether the VPN of this page still matches its PFN</li> <li>1 bit for modify(also called dirty bit): whether the page has been modified after it was brought into the memory. The dirty bit is set to 1 when change happens. The dirty bit is set to 0when the processor writes to (modifies) this memory. It is used to indicate a page has been modified but its change has not been written to swap file.</li> <li>3 bits for protection: whether the page could be read from, written to and execute</li> <li>1 bit for presence: whether the page is in physical memory or on the disk</li> <li>1 bit for reference: whether access to the page occurs. set to 1 when a read or write to the page occurs.</li> </ul> <p>Drawback of simple linear page table:</p> <ul> <li>Since each process keeps a page table for all of the pages in the memory, however, a process may not need all of the pages in memory and cause some entries in linear page tables to be invalid, thus waste space. A linear table occupies 2<sup>20</sup> * 4 bytes = 4MB. If 100 processes in OS, 400MB for the page tables</li> </ul>"},{"location":"csc369/#312-page-table-per-segment","title":"3.1.2 page table per segment","text":"<p>What is a segment:</p> <p>A segment is a continuous chunk of memory where most of the variables lie in</p> <p>Suppose the address space is a 32-bit virtual address space and the page size is 4KB:</p> <ul> <li> <p>Instead of allocating a single page for the entire address space, we allocate one page table for each used segment by the process</p> </li> <li> <p>the virtual address will allocate some bits for the segment number, VPN and offset(In the screenshot below, the process has 4 segments)</p> </li> </ul> <p></p> <ul> <li> <p>For each segment, we assign a pair of base register and limit register. The base register will store the start address of the page table of each segment instead of the start address of the segment. The limit will store the number of entries in the page table from the start entry to the last entry</p> </li> <li> <p>When the program gets the virtual address and wants to do address translation, this is the procedure:</p> </li> </ul> <pre><code>SN = (VirtualAddress &amp; SEG_MASK) &gt;&gt; SN_SHIFT\nVPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; VPN_SHIFT\nPhysicalAddress = Base[SN] + (VPN * sizeof(PTE))\n</code></pre> <p>Drawback of this method:</p> <ul> <li>There is fewer  page table waste than linear page table. However, there are still page table waste when each segment is very large and sparsely used</li> <li>External fragmentation for page tables. Since each page table can grow, finding free space for them is complicated</li> </ul>"},{"location":"csc369/#313-multi-level-page-table","title":"3.1.3 multi-level page table","text":"<p>structure of multi-level page table &amp; virtual address:</p> <p>virtual address:</p> <p></p> <ul> <li>The virtual address with multi-level page table is composed of VPN and offset bits. The VPN is composed of PD Index0, PD index1, PD Index2(Page Table Index in above screenshot), .... PD IndexN, if the number of levels is N + 1. The page table for the above virtual address is 3-level</li> </ul> <p>pagetable:</p> <ul> <li>The OS will store one page table for PD Index 0 level. The number of entries in the table = 2 to the power of (number of bits in PD Index 0). Each entry in this table points to the physical address of page tables in PD Index 1 level. After we computes the decimal value in PD Index 0, we use the value to index in level-0 page table to find the physical address of level-1 page table</li> <li>The entries of level-i page table point to the physical address of level-(i+1) page table. Thus, after we computes the value at PD Index i, we use the value to index the physical address of level-(i+1) table in level-i table</li> <li>The entries of level-N page table points to the physical address of the page where the data lies in. After we computes the value at PD Index N, we use the value to index the physical page addrss of the data in level-N table</li> <li>Suppose the number of bits for each PD Index i is n and all level page tables are full, then from PD Index 0 to N, we have 1 level-0 table, 2<sup>n</sup> level-1 tables, 2<sup>n</sup> * 2<sup>n</sup> = 2<sup>2n</sup> level-2 tables, ..., 2<sup>in</sup> level-i tables, ... 2<sup>Nn</sup> level-N tables. Thus, the number of level-N tables = number of entries in linear page table. The total  number of level-0 to level-(N-1) tables = 2<sup>Nn</sup> - 1. Both assume all level tables are full</li> <li>Due to the above complexity, we store 1 bit beside each PTE as the valid bit in level-0 tables to level-(N-1) tables. This bit indicates if the next level table pointed by the PTE contains a valid PTE that also points  to a valid next level table, until it points to a valid real data address. We initialize all of the valid bits as 0. When we allocate page for the process, we set the valid bits of the PTE to this page in level-N table as 1. We also set the valid bits of the PTE to this level-N table from level-0 table to level-(N-1) table as 1</li> <li>The multi-level page table saves more space than linear table when there are many pages are not used.  When many pages are not used, there are many empty invalid PTE in linear table and they waste space. However, we only allocate intermediate tables and the page tables when their valid bits are on. The total entries of these allocated tables are far more less than the whole linear table, since the size of each level table is far smaller than the linear table</li> </ul> <p>process of virtual address translation:</p> <p>Given:</p> <ul> <li>we have an N-level page table</li> <li>level ranges from 0 to N-1level and the i<sup>th</sup> level has k~i~ bits</li> <li>page size be 2<sup>M</sup> bytes</li> <li>a virtual address string with length \\(k_0 + k_1 + k_2 +... k_i + k_{N-1} + M\\) bits</li> </ul> <ol> <li>we first get the page table address from the page table register and get the level 0 mapping</li> <li>We translate the first \\(k_0\\) bits to decimal number \\(p_0\\), we look at the \\(p_0\\) position in level 0 table to find the physical address \\(PA_1\\)of level 1 page table. Now set i = 1</li> <li>we pull out the level i mapping. We translation \\(k_i\\) bits to decimal number \\(p_i\\), we look at  the p~i~ position in level i table to find the physical address \\(PA_{i+1}\\) of level (i+1) page table. We increment i and repeat step 3 till i = N-1 and the \\(PA_N\\) is the physical frame number</li> <li>We concatenate \\(PA_N\\) and offset bits </li> </ol> <p>how many levels should there be in multi-level page table:</p> <p>Given:</p> <ul> <li>the address space is 32 bits, </li> <li>the page size is 4KB(4096 bytes)</li> <li>32 bits architecture</li> </ul> <p>The steps to determine the number of levels and the bits for each level:</p> <ol> <li>32 bits architecture implies pte size is 32 bits(4 bytes) for each pte, 64 bits imply pte size is 8 bytes</li> <li>We first determine the offset bits: \\(\\(log_2(4096) = 12\\)\\) Thus, we allocate the rightmost 12 bits for offset and there are 32 - 12 = 20 bits for VPN, where the length of virtual address is 32 bits</li> <li>We determine how many PTEs can be in a page: </li> <li>4096 / 4 = 1024 </li> <li>\\(\\(log_2(1024) = 10\\)\\). </li> <li>20 // 10 = 2 thus, 2 levels</li> <li>20 % 10 = 0 (if the remainder great than 0, then the number of bits for PD Index 0 is the remainder; otherwise, it's log of the # of PTEs in a page)</li> </ol>"},{"location":"csc369/#314-hashed-page-tables","title":"3.1.4 Hashed page tables","text":"<p>structure of page table &amp; virtual address:</p>"},{"location":"csc369/#315-inverted-page-tables","title":"3.1.5 Inverted page tables","text":"<p>structure of page table &amp; virtual address:</p> <ul> <li>Keep one table for all processes instead of table for each process</li> <li>A hash table with the physical frame number as the key and list of tuples are the value. The tuple is composed of virtual number and process id of each process that uses this physical frame</li> </ul>"},{"location":"csc369/#4-tlb","title":"4.  TLB","text":"<p>TLB(Translation lookaside buffer):</p> <p>TLB is a quick cache that records the mapping from VPN to PFN for each process. TLB is not shared among processes. The OS first search TLB for translation before accessing page table. A TLB entry looks like:</p> <p>VPN | PFN |valid bits | protection bits\uff08read bits/write bits/execution bits) | other bits</p> <p>TLB type:</p> <ul> <li>software-loaded TLB(table must be in hardware-defined format) </li> <li>operations:<ul> <li>fault to OS, OS finds appropriate OTE, load in TLB</li> <li>add entry</li> <li>read entry</li> <li>invalidate entry</li> <li>invalidate all entries</li> </ul> </li> <li>hardware-loaded TLB(table can be in any format)</li> </ul> <p>Spatial locality:</p> <p>Suppose the program is accessing an array and all elements of the array live in the same page, then after the first TLB miss when accessing the first element, other elements are hit in TLB. Because the elements are packed tighly into pages(i.e: variables are close to one another in space). This is called spatial locality. Spatial locality refers to the use of data elements within relatively close storage locations</p> <p>Temporal locality:</p> <p>After the program accesses an array, it re-accesses the same array and these accesses increase he hit rate of TLB in program(i.e: quick re-referencing memory items in time). This is called temporal locality.</p> <p>Temporal locality refers to the reuse of specific data and/or resources within a relatively small time duration</p> <p>Management of TLB:</p> <ul> <li>When changing the bits of PTE(such as protection bits), we need to update the corresponding entry bits in TLB as well</li> <li>Reload TLB during process context switch</li> <li>TLB miss(TLB replacement policy)</li> </ul> <p>page fault:</p> <ol> <li>When it evicts a page, the OS sets the PTE as invalid and stores the location of the    page in the swap file in the PTE</li> <li>When a process accesses the page, the invalid PTE will cause a trap (page fault)</li> <li>The trap will run the OS page fault handler</li> <li>Handler uses the invalid PTE to locate page in swap file</li> <li>Reads page into a physical frame, updates PTE to point to it</li> <li>Resume process</li> </ol>"},{"location":"csc369/#5-page-movement-policy","title":"5. page movement policy","text":"<p>sFetch Policy: when to fetch a page</p> <ul> <li>Demand Paging</li> <li>Prepaging</li> </ul> <p>Placement Policy: where to put the page</p> <ul> <li>NUMA (non-uniform memory access) </li> <li>Cache performance</li> </ul> <p>page replacement policy: what pages to evict</p> <p>page eviction\uff1a</p> <p>This just means delete a page from the memory, may not need to write to the disk</p> <p>common problem of page replacement:</p> <ul> <li>thrashing:  more time is spent by OS in paging data back and forth from disk than executing user programs</li> <li>solution1: swap all pages of a process into disk and suspend it</li> <li>soln2: buy more memory</li> </ul> <p>Operation of page movement algorithm:</p> <ul> <li> <p>access: lookup and access the page in memory</p> </li> <li> <p>insert: add the page to the memory frame when there is empty frame</p> </li> <li> <p>replace: evict the page frame and insert the page to the evicted frame mapping when no memory frame is available</p> </li> </ul>"},{"location":"csc369/#51-fifo","title":"5.1 FIFO","text":"<ul> <li> <p>Maintain a list of pages in order in which they were paged in</p> </li> <li> <p>operation:</p> </li> <li> <p>insert: add to the head of queue</p> </li> <li>replace: remove the page at the tail of the queue and insert the new page to the head of the queue</li> </ul> <p></p>"},{"location":"csc369/#52-exact-lru","title":"5.2 exact LRU:","text":"<ul> <li> <p>use a doubly linked list to keep pages. We change the data structure in following:</p> </li> <li> <p>when we insert: add page to the top</p> </li> <li>when we access a certain page: we traverse the linked list, find the page and move to the top</li> <li>when we replace and the list is full: we evict remove the page at the bottom and then insert at the head</li> </ul> <p></p>"},{"location":"csc369/#53-additional-reference-bit","title":"5.3 Additional Reference bit","text":"<p>Data structure: Initially, allocate R reference bits to each page. All of the R bits of each page are initially set to 0. We change the data structure at when:</p> <ul> <li>insert: When we insert the page, we find the page and set only the leftmost bit to 1.</li> <li>access: When we access the the page, we find the page and set only the leftmost bit to 1.</li> <li>periodically: at regular interval, treat the R reference bits as a binary string and right shift this binary string one bit to the right, for example, if reference bits is 1000, then it becomes 0100; if it's 0110, then it's 0011</li> <li>replace: for each page, decode its reference bits as a decimal digit and choose the page that has the smallest decimal digit to evict, and then replace with the new page, initialize the R reference bits all to 0, set the leftmost reference bits of the new page 1</li> </ul>"},{"location":"csc369/#54-second-chance-algorithmclock-algorithm","title":"5.4 Second chance algorithm(clock algorithm):","text":"<p>pseudo:</p> <ol> <li>Arrange the physical frame in a circular list like a clock and let the clock pointer points to the first frame</li> <li>A new process is created and needs to be written to the frame</li> <li>If the frame pointed by the clock pointer has not been occupied by a process:</li> <li>if the process appears in the circular list, then do nothing</li> <li>else:<ol> <li>store the new process in the frame pointer by the clock pointer</li> <li>set the reference bit of the frame pointed by clock pointer to 1</li> <li>increment clock pointer to point to the next frame</li> </ol> </li> <li>else:</li> <li>if the process appears in the circular list <ol> <li>if its reference bit is 1, then do nothing</li> <li>else: set the process's frame reference bit to be 1(do not increment clock this time)</li> </ol> </li> <li>else: <ol> <li>if the reference bit of the frame is 1:<ol> <li>set the reference bit to 0</li> <li>increment the clock pointer to point to next frame</li> <li>goto the step 2 in else block labeled in step 2</li> </ol> </li> <li>else:<ol> <li>evict the old process in the frame and write the new process to this frame</li> <li>set the reference bit in the frame to 1</li> <li>increment the clock pointer to point to next frame</li> </ol> </li> </ol> </li> <li>A new process is created and re-execute step 2</li> </ol> <p>example:</p> <p></p> <p>We first create a circular list from the frame according to step 1: 0 -&gt; 1 -&gt; 2 -&gt; 3 -&gt; 0 and let clock points to frame 0</p> <p>AFC: Since frame 0 is empty and process AFC is not in the list, we put in frame 0 and increment clock to 1</p> <p>008: same as above</p> <p>BC0: same as above</p> <p>AFC: Since frame 3 is empty and AFC is in the list, we do nothing</p> <p>110: Since frame 3 is empty and 110 is not in list, we put in frame 3 and increment clock back to 0</p> <p>008: Since frame 0 is not empty and 008 is in list and 008 reference bit is 1, do nothing</p> <p>AEE: Since frame 0 is not empty and AEE is not in list and ref bit in frame 0 is 1, we set ref bit off and increment clock till back to frame 0. Then we evict AFC, set ref bit on in frame 0 and increment clock to frame 1</p> <p>AFC: Since frame 1 not empty, AFC not in list and ref bit in frame 1is 0, we evict 008 at frame 1, set ref bit in frame 1 to 1 and increment clock to frame 2</p> <p>BC0: Since frame 2 not empty and BC0 in list and ref bit of BC0 is 0, we set its ref bit to 1</p> <p>008: Since frame 2 not empty, 008 not in list and ref bit in frame 2 is 1, we set ref bit off and increment clock to frame 3. Thn we evict 110, set ref bit on in frame 3 and increment clock to frame 0</p>"},{"location":"csc369/#55-beladys-algorithm","title":"5.5 Belady's algorithm:","text":"<p>The algorithm assumes we know what pages will come out at any time in the future instead of having to wait till that moment the page comes out. For example, before any pages are loaded to the memory, we know beforehand the page sequence order is: <code>l</code> = [2, 3, 2, 1, 5, 2 , 4, 5, 3, 2, 5, 2].</p> <p>To use Belady's algorithm, we need to be given the page sequence  <code>l</code> and total number of frames(say 3), after the frames are all filled with memory at <code>i</code>(say <code>i</code> &gt; 3) and we want to evict a page in the frames and replace that page with <code>l[i]</code>, we:</p> <ol> <li>Among all pages in the frame, check if there is a page that does not appear after <code>l[i]</code>. If so, we evict this page and replace with <code>l[i]</code>. If not, we proceed</li> <li>For all the pages in the frame, we check their next presence after <code>l[i]</code>. We select the page with the largest index after <code>i</code> </li> </ol> <p>Explanation:</p> <p>Suppose the frame is filled with 2, 3, 1 and the next page is 5(<code>i</code> = 4):</p> <p><code>i = 4</code>: the frame is 2, 3, 1. Since there is no presence of 1 after <code>l[4]</code> in the list of <code>l</code>, we evict page 1. The frame is now: 2, 3, 5</p> <p><code>i = 5</code>: No need to evict, since <code>l[5]</code> is 2 and it's in the frame</p> <p><code>i = 6</code>: The frame is now: 2, 3, 5. Since the next presence of 2 is at l[9], the next presence of 3 is at l[8], the next presence of 5 is at l[7]. The index of 2(which is 9) is the largest, we evict page 2. Thus, the frame is: 4, 3, 5</p> <p>.......</p>"},{"location":"csc369/#56-simple-2q","title":"5.6 Simple 2Q","text":"<p>We have 2 queues to store all pages: one called is A1 and the other is called Am. A page is in ether of the 2 queues. We change the data structure when:</p> <ul> <li>access/insert a page called p:</li> </ul> <p>if p in Am queue:</p> <p>\u200b   move p to the top of Am</p> <p>elif p in A1 queue:</p> <p>\u200b   remove p from A1</p> <p>\u200b   put p at the top of Am</p> <p>else: # p is a new page and we insert p(not access)</p> <p>\u200b   put p at the top of A1</p> <ul> <li>Replace a page:</li> </ul> <p>if A1\u2019s size is above its threshold</p> <p>\u200b   evict oldest page from A1 (first-in)</p> <p>\u200b   put p in the freed frame</p> <p>else:</p> <p>\u200b   delete LRU page from Am</p> <p>\u200b   put p in the freed frame</p>"},{"location":"csc369/#58-lfu","title":"5.8 LFU","text":""},{"location":"csc369/#59-mfu","title":"5.9 MFU","text":""},{"location":"csc369/#510-page-buffering","title":"5.10 Page Buffering","text":""},{"location":"csc369/#7-vma","title":"7. VMA","text":"<ul> <li>what is VMA</li> <li>how is VMA managed</li> </ul> <p>What is VMA:</p> <p>VMA is a virtually contiguous chunk of memory. It is different from segment in that the virtual address inside VMA is continuous, but the physical address of the pages inside the VMA may not be continous. The reason for virtually continuous is to allocate new area easily. The VMA for each process could be the code area, data area, stack area.</p> <p>how is VMA managed:</p> <ul> <li> <p>creation: </p> </li> <li> <p>For each process, some VMAs are initialized through <code>exec()</code> as stack, code space, data space</p> </li> <li> <p>additional VMAs are created through <code>malloc()</code>, <code>mmap()</code>, <code>sbrk()</code>. Process of creation of VMA: ...</p> </li> <li> <p>How is it being tracked: As the screenshot below, task struct represents each process information. The mm struct represents memory management of each process. The vm_area struct represents each single VMA</p> </li> </ul> <p></p>"},{"location":"csc369/#8-how-much-memory-to-allocate-for-each-process","title":"8. how much memory to allocate for each process","text":"<ul> <li>Fixed space algorithm(Local replacement): Each process is given a fixed limit of pages to use</li> <li>Variable space algorithm(Global replacement):  Process's set of frames can grow and shrink dynamically</li> </ul>"},{"location":"csc369/#9-working-set-model","title":"9.  Working set model","text":"<p>Defn:</p> <ul> <li> <p>WS(t, \\(\\(\\Delta\\)\\)) = set of pages that are referenced in the time interval (t - \\(\\(\\Delta\\)\\), t]</p> </li> <li> <p>t = time, \\(\\(\\Delta\\)\\) = number of page references</p> </li> </ul> <p>Ex:</p> <p></p>"},{"location":"csc369/#10-sharing","title":"10.  Sharing","text":"<p>What sharing refers:</p> <p>shared memory to allow processes to share data using direct memory references</p> <p>how to implement:</p> <ul> <li>Have PTEs in both tables map to same physical frame, same or different virtual page number in each process's address space map to the same physical frame number</li> <li>different:  flexible(no address space conflicts), but pointers inside the shared memory segment are invalid if the pointer references an address outside the segment(if we decode a virtual address stored in the shared mmory segment, it will map to different physical frame number to each process and the dereference this pointer will give different results)</li> <li>same: less flexible, but the pointers are valid</li> <li>Each PTE can have different protection values</li> <li>Must update both PTEs when page becomes invalid</li> </ul>"},{"location":"csc369/#11-copy-on-write","title":"11. copy on write","text":"<p>Background(When do we need CoW)\uff1a</p> <p>OS spends time in copying data. Two common occasions: One is copy happens in the movement of system call arguments between user and kernel space. The other one is the copy of the entire address space in fork() which is the most important case. CoW comes out to defer copy so that time can be saved in copying pages</p> <p>What is CoW(how is CoW implemented in fork):</p> <p>It is a technique when a child process wants to copy parent pages:</p> <ul> <li>Instead of copying pages, the child process create shared mapping to the parent physical pages in child process's page table in child process virtual address space</li> <li>Shared pages are protected as read-only in child and write permission to the parent pages are removed for parent process as well(i.e: parent pages become read only to both parent and child)</li> <li>When either parent process or child process(i.e: say child) wants to write to a particular frame pointed by a particular page:</li> <li>The write generates protection fault and trap to OS</li> <li>The OS allocates a new physical frame and copy the particular page to this new frame</li> <li>The OS updates the page table in child process to map the page number to the new physical frame number</li> <li>It adds the write permission of the page number back in the parent's page table</li> </ul>"},{"location":"csc369/#12-mapped-files","title":"12. mapped files","text":"<p>What is mapped files:</p> <p>It is a technique to cache file data in memory for accessing instead of having to call read() and write(). This is faster than the two system calls. The mapped files can be achieved through the mmap() system call</p> <p>The mechanism of <code>mmap()</code>:</p> <ul> <li>the <code>mmap()</code> creates a VMA for the file open by mmap as the last VMA in screenshot below:</li> </ul> <p></p> <p>This VMA has extra field called <code>ops</code> and <code>file</code>. <code>ops</code> is a function pointer that allows us to handle operations on different portions of the virtual address space in different ways, ex include page in function and page out function, etc. The file field indicates where Pagein/Pageout ops apply to and they will be page out to a named file instead of  a swapfile.</p>"},{"location":"csc369/#file-system","title":"File System","text":""},{"location":"csc369/#1-file-conceptual-operation","title":"1. File Conceptual Operation","text":""},{"location":"csc369/#2-file-access-operation","title":"2. File access operation","text":"<ul> <li>General-purpose file system</li> <li>Sequential access: read a byte at a time in orde(Unix, NT does)</li> <li> <p>Direct Access: random access given block/byte or number</p> </li> <li> <p>DB</p> </li> <li>Record access</li> <li>Indexed access</li> </ul> <p>command:</p> <ul> <li>cat</li> </ul> <p>System call:</p> <ul> <li>link</li> <li>unlink</li> <li>open</li> <li>read</li> <li>write</li> <li>rename</li> <li>mkdir</li> <li>rmdir</li> <li>rm</li> <li>truncate</li> </ul>"},{"location":"csc369/#3-file-directory-link-object-structure","title":"3. File, Directory, link object structure","text":"<p>block: </p> <ul> <li>Disk space is allocated in the granularity of blocks, must be a multiple of disk block size (e.g., typical size 4KB file system block size = 8 * 512 byte disk sector size)</li> </ul> <p>File:</p> <ul> <li>File is a struct that contains filename, inode number, which is a unique identifier associated with File and other properties</li> <li>to simplify, just view it as a tuple of the (fileName, InodeNumber)</li> <li>how to allocate data blocks for a file:</li> <li>Contiguous Allocation: applicable to read-only file system</li> <li>Linked Allocation: each file data block has a disk pointer to the next data block</li> <li>Indexed allocation: has inode for each file as the index to the data block of the file. The inode records each data block of the file</li> <li>Extent Based Allocation: has inode for each file as the index to the data block of the file. The inode additionally records the start data block and the number of data blocks from the start data block to the end data block.</li> </ul> <p>Directory:</p> <ul> <li>Directory contains name, inode number and a list of tuples of (name, inodeNumber) that represents the content of the directory</li> </ul> <p>How the file system data structure works:</p> <p></p> <ul> <li>The file system first remembers the Directory Struct of \"/\" directory. This struct contains the list of tuple of (name, inodeNumber) that represents the content of its sublevel. Then from each inodeNumber from the tuple of the list, it finds the Struct Directory or Struct File from the inodeNumber. This process then continues.</li> </ul>"},{"location":"csc369/#4-file-system-implementation","title":"4. File system implementation","text":"<p>File system implementation has the following type:</p> <ul> <li>vsfs(very simple file system)</li> <li>Windows: NTFS</li> <li>Mac OSX: HFS+</li> <li>BSD, Solaris: UFS, ZFS</li> <li>Linux: EXT2, EXT3, EXT4, F2FS, XFS, BTRFS, etc.</li> <li>Check out: https://en.wikipedia.org/wiki/List_of_file_systems</li> </ul>"},{"location":"csc369/#41-vsfs","title":"4.1 VSFS","text":"<p>vsfs is a simplified version of Unix file system</p> <p></p> <p>VSFS:</p> <ul> <li>A disk drive is consisted of block(4KB). Suppose the drive has 64 bytes as the pic above, then a vsfs consists of </li> <li>a superblock,  denoted as S in above pic</li> <li>inode bitmap, denoted as i in above pic(made from 1 block), </li> <li>data bitmap, denoted as d in above pic(made from 1 block), </li> <li>Inodes table, denoted as I in above pic(made from some blocks, ex: 5 blocks in the pic)</li> <li>Data region, denoted as D in above pic(made from rest of blocks, ex: 56 blocks in the pic)</li> </ul> <p>SuperBlock:</p> <ul> <li> <p>Always at a well-known disk location</p> </li> <li> <p>Record how many blocks for Inode tables and at which block does the Inode tables start, file system version</p> </li> </ul> <p>limit: we can have free inodes number larger than the free data block number</p> <p>variation #1: index-based </p> <p>Inode table:</p> <ul> <li>A collection of Inode(Inode is the data struct for meta data of file)</li> <li>each Inode has the following important info:</li> <li>Inode number(a unique identifier for the Inode in Inode table, this info can be used to calculate the address of location of the inode in the disk)</li> <li>Finite direct pointer(typically 12 pointers,link the inode to the location of the file data in the Data region)</li> <li>Indirect pointer</li> <li>instead of pointer, inode can store extent</li> </ul> <p>Data Region:</p> <ul> <li>data region  stores either the content of file, indirect blocks for files or directory structure</li> <li>File: stores the content of the file</li> <li>Director structure: stores a list of entries. Each entry contains the name of the file or sub-directory under that directory, the inode of that file or sub-directory, length of the name and record length</li> </ul> <p>variation #2: extent:</p> <p>What is extent:</p> <p>An extent == a disk pointer plus a length (in # of blocks). Instead of requiring a pointer to every block of a file, we just require a list of (start block, length) tuples</p> <p>Processes of the file system(how file system works):</p> <ul> <li>Create a file or directory</li> <li>search through the inode bitmap for free inode number</li> <li>Allocate that inode for the file on that inode number</li> <li>Mark the inode number as used in inode bitmap. If the user writes sth, then:<ol> <li>search through the data bitmap for free data block</li> <li>Allocate the data block for the file and write contents to the data block</li> <li>Mark the data block as used in the data bitmap</li> <li>Modify the inode to point to the data block</li> </ol> </li> <li>Add an entry for the file in its parent directory data block to link it</li> <li>Inode block is modified for parent directory</li> <li>Read a file or directory:</li> <li>The open() pulls out the inode of directory \"/\" from its well known inode number(typically 2)</li> <li>Read its data block to search the subdirectory entry name and inode number through \"/\" inode's direct pointer and traverse to the corresponding file</li> <li>The FS calls open() to read the file's inode into memory, check its permission,  allocates a file descriptor for this process in the per-process open-file table</li> <li>The read() call will find the data block of the file through inode's direct pointer or indirect pointer</li> <li>The close() will dellocate file descriptor </li> <li>append a file(direct I/O, quite complicate):</li> <li>allocate a new data block on the dbmap</li> <li>write the content to the new data block</li> <li>update the file's inode block to point to the new data block</li> <li>delete a file or directory:</li> <li>cached</li> <li>static partitioning</li> <li>dynamic partitioning</li> </ul> <p>Drawback &amp; Problem:</p> <ul> <li></li> </ul> <p>As a result, file F is not continuous and gap from F2 to F3 might lead to seeks and positioning</p> <ul> <li> <p>Since Inodes allocated far from blocks, when we traverse the file path, directories need to go back and forth from inode tables to data region and increases a lot of seeks(seek refers to the movement from one track to the other on an HDD hard disk)</p> </li> <li> <p>Inefficient for transfer of block data</p> </li> </ul>"},{"location":"csc369/#42-ffs-disk-aware-fs","title":"4.2 FFS: disk-aware FS","text":"<p>Background: Since vsfs has lots of seeks, FFS arise to reduce the number of seeks. FFS is the core idea of ext2, ext3 file system</p> <p>Concepts need to know beforehand:</p> <p></p> <ul> <li>sector: A small segment of the ring in the screenshot</li> <li>track: Each ring at different height in the pic. There are 4 rings with 4 diff colors in the pic</li> <li>Cylinders: Tracks at same distance from center of drive at different heights</li> <li>Cylinder groups: Set of N consecutive cylinders </li> </ul> <p>What is FFS</p> <p>FFS is mostly the same as VSFS, which is composed of superblock, inode bitmap, data bimap, inode tables, data region. The difference between FFS and VSFS is that:</p> <ul> <li>FFS divides the HDD hard drive into cylinder groups. Each cylinder group has superblock, inode bitmap, data bimap, inode tables, data region, </li> <li>while there is only one group of superblock, inode bitmap, data bimap, inode tables, data region, where we don't care cylinder group, in vsfs</li> </ul> <p>Behaviour difference:  Read is the same as vsfs. Compared with VSFS, ffs has complex allocation strategy when writing to disk:</p> <ul> <li>Goal:</li> <li>Closeness:<ul> <li>reduce seek times by putting related things in one cylinder group as much as possible or putting things in close cylinder group</li> </ul> </li> <li>Amortization:<ul> <li>Amortize each positioning delay by grabbing lots of useful data</li> </ul> </li> <li>Policy:</li> <li>Data blocks in same file allocated in same cylinder group</li> <li>Files: Files in same directory allocated in same cylinder group</li> <li>Files &amp; directory: Inodes for files allocated in same cylinder group as file data blocks</li> <li>When allocating a large file, break it into large chunks and allocate from different     cylinder groups, so it does not fill up one cylinder group</li> <li>If preferred cylinder group is full, allocate from a \u201cnearby\u201d group</li> </ul> <p>Advantage over VSFS:</p> <ul> <li>Although fragmentation may still exist in aging FFS, since data blocks of the same file are in cylinder group in most cases, seeks reduce</li> <li>Since inode tables and data blocks of the same file are in cylinder group in most cases, movements from inode to data are greatly reduced</li> </ul>"},{"location":"csc369/#43-ntfs","title":"4.3 NTFS","text":"<p>https://recoverit.wondershare.com/file-system/ntfs-file-system.html</p> <p>Like vsfs, which is composed of superblock, 2 bitmaps, inode tables and data region.</p> <p>The NTFS is composed of: </p> <ul> <li>partition Boot sector</li> <li>Master File table</li> <li>system files</li> <li>User files</li> </ul> <p></p> <p>MFT:</p> <ul> <li> <p>A table that contains several records for files &amp; directories</p> </li> <li> <p>Each MFT record is a sequence of variable length (attribute header, value) pairs</p> </li> </ul> <p></p> <ul> <li></li> <li></li> </ul>"},{"location":"csc369/#44-vfs","title":"4.4 VFS","text":""},{"location":"csc369/#45-lfs","title":"4.5 LFS","text":"<p>This is the basis of the F2FS file system. The disk layout of LFS is composed of CR and multiplesegments:</p> <ul> <li>checkpoint region(CR): This is a fixed and known position in disk that stores the address to the latest imap. CR region is updated periodically.</li> <li>segments: Each segment is composed of data blocks of a file and updated inode block of that file next to the data block and the one imap in memor when the segment is written to the disk</li> <li>imap: This is an array with 4 bytes(disk pointer) per entry. Since the inode block is scattered in segments of the disk, the imap stores the address of the inode block. The i<sup>th</sup> position of the imap stores the i<sup>th</sup> inode address. Each segment has a different version of imap</li> </ul> <p>How does LFS work</p> <p>Unlike the above fs, which will write the inode blocks, bitmap blocks and data blocks for each a file at a fixed and discrete position in disk, the LFS actually writes a large and continuous chunk from the buffer once the buffer is full. This chunk is called segment and it is composed of updated data block, inode blocks and imap of multiple files and their parent directories. The below screenshot describes the layout of a segment when the segment is written to the disk. The file and its parent directory blocks lie in the segment</p> <p></p> <p>Therefore, the inode blocks and data blocks are not in a separate region as the inode table regions and data regions in FFS or other fs. The reason for writing the inode block and data block in one segment chunk on disk is to make advantage of sequential write of hard disk. In addition, when there is another new update of the file in the screenshot, LFS will not overwrite the location of the above segment. Instead, it will write a new segment of the file at a different location. Thus, we say the location of the data block and inode block of a file is not fixed in LFS. Periodically, the LFS will do garbage collection to clean the previous segment on disk</p> <p>Different processes of LFS:</p> <ul> <li> <p>How does LFS write a new file to disk or append a data block to a file:</p> </li> <li> <p>To create a new file or allocate new data block to a file, we will need to create new data block, new inode block to point  to the data block for the file. We also need to create new data blocks for its parent directories to include the new file entry and the inode block for the parent directory to point to the data block.  The LFS will first update the 4 writes in the buffer and change the address of the inodes location in the imap in memory</p> </li> <li> <p>Once buffer is full, it will write the data blocks and inode block of the same file next to each other in the segment, as the below screenshot. Then it will write out the imap in memory to the disk to record the newest location of the inodes</p> <p></p> </li> <li> <p>After a while, LFS will update CR region to point to this latest imap location periodically</p> </li> <li> <p>After some while, LFS will do garbage collection periodically in the case of appending a data block to a file. (ie: since LFS write a new inode block to point to two data blocks at new locations but do not erase the old inode block for that file in the below screenshot, the LFS has to do garbage collection)</p> </li> <li> <p>How does LFS modify the file's data block:</p> </li> <li> <p>To modify the data block of a file, we actuall creates a new data block for that file instead of writing to the old data block. We also create new inode block to point to that data block. The LFS also change the address of the inodes location in the imap in memory</p> </li> <li> <p>Once buffer is full, the LFS will write the the new data block and new inode block to the segment in disk. The disk layout will be like screenshot below. At the end, it will append the imap to the segment</p> <p></p> </li> <li> <p>After a while, LFS will update CR region to point to this latest imap location periodically</p> </li> <li> <p>After some while, LFS will do garbage collection periodially to clean both garbage</p> </li> <li> <p>How does LFS read a file:</p> </li> <li>Suppose the imap is not in the memory, it will first read the checkpoint to read the imap into memory</li> <li> <p>During the process of directory traversal and file read, it will find the inode location in the imap</p> </li> <li> <p>Process of Garbage collection:</p> </li> </ul> <p>Periodically, LFS will do the following in order:</p> <ol> <li>LFS take M old(partially-used) segment and determine the live data blocks in these M segments one segment by segment(how to determine live blocks below:)<ol> <li>For data block D located at address A, look into the segment summary block(stores the inode number and offset for each data block) to find inode number N and offset T</li> <li>Look in the N<sup>th</sup> entry of imap to find the inode</li> <li>Read the inode and check the T<sup>th</sup> data block pointer in the inode. If the block pointer points to address A, the block is live. Otherwise, it's dead</li> </ol> </li> <li>LFS write the live blocks into N(N &lt; M) new segments(which segments to choose: cold then the hot)</li> <li>LFS then freed the M old segments</li> </ol> <p>Advantage of LFS:</p> <ul> <li>make use of sequential write well on HDD; since the data block and inode block are next to each other in segments, it also makes use of sequential read on HDD</li> </ul>"},{"location":"csc369/#46-fat-32-file-system","title":"4.6 FAT-32 file system","text":""},{"location":"csc369/#5-crash-consistency-problem-sln","title":"5. Crash-consistency problem &amp; Sln","text":"<p>What is the problem:</p> <p>When the file system tries to update the persistency(ex: append data block to a file, create a file), which requires update of two meta data structures A and B to complete the operation, suppose power loss or system crashes after one write of A completes, the other data structure B will be left in an inconsistent state. This problem is called crash consistency problem</p> <p>Soln to the problem:</p> <ul> <li>Uninterruptible power supply (UPS)(Doesn\u2019t help if failure is due to system crash)</li> <li>fsck(file system checker)</li> <li>journaling(important focus)</li> </ul> <p>Example(in-consistency problem):</p> <p>Suppose the fs tries to append a data block to an existing file. The fs will update its inode pointer to point to the new data block, update the data block in data bitmap and content of data block. Thus, there are three writes and the order is random.</p> <p>If only one write is successful, crash scenario:</p> scenario severity Only data block is writen This is as if the write never happens and No  crash-inconsistency problem only updates Inode - read garbage data - multiple inodes may eventually point to the same data blocks- data bitmap shows not allocated, while inode shows it is, leading file system crash inconsistency Only updates data bitmap - bitmap shows block used, leading to space leak- data bitmap shows allocated, while inode shows it is not, leading file system crash inconsistency <p>If two writes successful, crash scenarios:</p> scenario severity Only inode (I[v2]) and bitmap (B[v2]) are written file points to garbage data Only inode (I[v2]) and the data block (Db) are written - file points to garbage data- Multiple inodes may eventually point to the same data block Only bitmap (B[v2]) and data block (Db) are written Data leak (data block is lost for any future use)"},{"location":"csc369/#51-fsck","title":"5.1 FSCK","text":"<p>What is FSCK:</p> <p>It is a program that scans the entire file system and checks any inconsistency problems by checking the following:</p> <ol> <li>Superblock: sanity checks</li> <li>Free blocks: scan inodes (including all indirect blocks), build in-memory bitmap</li> <li>Inode state: check inode fields for possible corruption</li> <li>Inode links: verify links count for each inode</li> <li>Duplicates: check if two different inodes refer to the same block</li> <li>Bad blocks: bad pointers (outside of valid range)</li> <li>Directory checks: integrity of directory structure</li> </ol> <p>limit:</p> <ul> <li>only verify metadata is consistent and does not check corruption at data block</li> <li>scan the file system makes it slow</li> </ul>"},{"location":"csc369/#52-journaling","title":"5.2 Journaling","text":"<p>What is journaling &amp; How does it help:</p> <p></p> <p>The file system also allocates a section of blocks for journal purpose as well as the section of superblock, inodetables, bitmap. Before the file system updates multiple meta data structures, the file system firsts backup the content of what the file system is going to write to the data structure and wraps up the content in a transaction in the journal section of the disk. Then it updates the file system meta data. Then the system crash may happen during the updates of journal section or meta data. This approach will have different effects(either recover data or skip) based on which of the previous two situations to make the file system structure consistent.</p> <p>For journaling, we have following different type of journaling:</p> <ul> <li>data journaling</li> <li>metadata journaling </li> </ul> <p>What is data journaling:</p> <p>If the file system adopts data journaling and is going to write to the disk(ex: create a new file, append data block to a file), the fs will have the following operations:</p> <ol> <li> <p>Journal write: Write the contents of the transaction (including TxB, metadata, and data) to the log; wait for these writes to complete.</p> </li> <li> <p>Journal commit: Write the transaction commit block (containing TxE) to the log; wait for write to complete; transaction is said to be committed.</p> </li> <li> <p>Checkpoint: Write the contents of the update (metadata and data) to their final on-disk locations</p> </li> <li> <p>Free: Some time later, mark the transaction free in the journal by updating the journal superblock  </p> </li> <li> <p>Why do we need a separate step 2:</p> </li> </ol> <p>Suppose we don't separate step 2 and combine step 2 with step 1, then it is possible that the user data is the last write in journal write, since the order of TxB, metadata, data and TxE is not controlled. If system crash happens before the write of data write and TxB, metadata and TxE are completed in the journal, then during the checkpoint recovery step, garbage may be written to the data blocks</p> <ul> <li>How data journaling reacts with system crash scenario:</li> </ul> scenario how data journal react crash during journal write Nothing will be recovered and file system data not updated crash after journal write and before journal commit Nothing will be recovered and file system data not updated crash during journal commit Impossible to happen, since any 512-byte write is guranteed to be atomic in disk crash after journal commit Transactions in journal will be replayed and file system recovery will happen <p>What is metadata journaling:</p> <p></p> <p>If the file system adopts metadata journaling and is going to write to the disk(ex: create a new file, append data block to a file), the fs will have the following operations:</p> <ol> <li> <p>Data write: Write data to final location; wait for completion (the wait is optional; see below for details).</p> </li> <li> <p>Journal metadata write: Write the begin block and metadata to the log; wait for writes to complete.</p> </li> </ol> <p>(Note: if we are changing the content of the directory that lies in the data region of the disk, the content of the drectory is also counted as metadata)</p> <ol> <li> <p>Journal commit: Write the transaction commit block (containing    TxE) to the log; wait for the write to complete; the transaction (including data) is now committed.</p> </li> <li> <p>Checkpoint metadata: Write the contents of the metadata update    to their final locations within the file system.</p> </li> <li> <p>Free: Later, mark the transaction free in journal superblock  </p> </li> <li> <p>Why is Data write the first step or can be the second step:</p> </li> </ol> <p>Suppose Data write does not happen in the first step and happen after journal commit step, assume system crashes right after Journal commit step and before data write, then data write does not succeed and the data is not in the journal. During the step of checkpoint, the inode in journal metadata will point to garbage data</p> <ul> <li>advantage over data journaling</li> </ul> <p>Comparing with data journaling, metadata journaling does not have to write data in journal section, which may reduce lots of disk traffic(since user data may be large)</p> <p>How Journal section manages transactions:</p> <p>Each journal log is wrapped in the data structure of transaction. The journal section records each journal in circular log:</p> <p>\u200b   </p> <ul> <li>transaction: The transaction is composed of 3 parts. The first part is the transaction start block(TxB), which contains transaction identifier(TID), the addresses of the inode table block, bitmap block and data block. The middle part contains the updates of the file system metadata and user data. The last part contains transaction end block(TxE)</li> </ul> <p></p> <ul> <li>circular log: In the journal section, we mark the oldest and newest non-checkpointed txns in the log as journal superblock. When we free the txn in journal section, we just mark the txn free in journal by updating the journal superblock.</li> </ul> <p>Why does journal writing break sequential write?</p> <p>Jump back-and-forth between writes to journal and writes to main region</p> <p>Corner case of journaling and soln</p> <ul> <li>Situation: Suppose fs uses metadata journaling. Suppose user A creates a file in directory foo and the location of foo directory data block is 1000. Thus, an entry is added to the directory foo and a write to block 1000 will happen. The user A completes the step of journal commit, the inode of foo directory and data of foo directory will be in journal. The user B deletes foo directory in another process, thus freeing block 1000. Then he creates another file foobar, which by accident reuses block 1000. At this time, user B completes his step of journal commit, the inode of foobar directory will be in journal. The journal will be in the screenshot below</li> </ul> <p></p> <p>Now suppose system crashes after both steps of journal commit. When we replay the two transactions above and recover the data, the order of recovery using user A's journal transaction or B's journal transaction lead to different consequences)</p> <ul> <li> <p>Reason for this: Deleting free up the disk block, leading to the oppotunity for two different processes using the same block at the same time</p> </li> <li> <p>Solution:</p> </li> <li> <p>never reuse blocks until the delete of said blocks is checkpointed     out of the journal  </p> </li> <li>add a new type of record to the journal, known as a revoke record.  Deleting the directory would cause a revoke record to be written to the journal. When replaying the journal, the system first scans for such revoke records; any such revoked data is never replayed  </li> </ul>"},{"location":"csc369/#ssd","title":"SSD","text":"<p>SSD is composed of the following:</p> <ul> <li>non-persistent memory(eg: SRAM): cache and buffer data as well as for mapping tables</li> <li>flash translation layer(FTL controller): converts read and write request to read, erase and program operations in SSD</li> <li>flash chips</li> </ul> <p>Each flash chip is composed of </p> <ul> <li>block(ex: 128KB or 256KB): erase units</li> <li>Each block is composed of page(ex: 4KB)</li> </ul>"},{"location":"csc369/#1-ssd-algorithm","title":"1. SSD algorithm","text":"<ul> <li>read</li> </ul> <p>SSD has invariation in page write algorithm. We have:</p> <ul> <li>naive way</li> <li>optimized way</li> <li>Page-based FTL mapping</li> <li>block-based FTL mapping</li> <li>hybrid</li> </ul> <p>The following sections will be algorithm with different sections</p>"},{"location":"csc369/#11-naive-write","title":"1.1 Naive write","text":"<ul> <li>write a page(naive approach):</li> <li>Find the block containing the target page</li> <li>if the page is erased:   // write new contents to this page<ol> <li>write the page directly</li> </ol> </li> <li>else:   // this page is valid and we need to update this page<ol> <li>Read all active pages in the block into controller memory</li> <li>Update target page with new data in controller memory</li> <li>Erase the block (high voltage to set all bits to 1)</li> <li>Write entire block(the updated target pages + other unupdated active pages) to drive</li> </ol> </li> </ul>"},{"location":"csc369/#12-optimized-write-with-page-based","title":"1.2 optimized write with page-based","text":"<ul> <li> <p>write a page(optimized approach):</p> </li> <li> <p>Prepare a mapping from logical page number to physical page number in memory</p> </li> <li> <p>Treat the physical pages as a sequence of page or log or array. </p> <p>When we are given a logical page number to write</p> </li> <li> <p>look through the mapping and check if it exists in mapping.</p> <p>if not:</p> <ol> <li>allocate a physical page at the end of the log(if this block has not been erased, erase it)</li> <li>write contents to the physical page</li> <li>add the logical page number and this physical block to the mapping</li> </ol> </li> <li> <p>else:   // this page is valid and we need to update this page</p> <ol> <li>allocate a physical page at the end of the log(if this block has not been erased, erase it)</li> <li>write contents to the physical page(No need to erase and write the valid page back comparing with the naive approach)</li> <li>update the mapping of the logical page number to point to this physical page number</li> </ol> </li> <li> <p>GC:</p> </li> <li>Periodically, it trims some garbage pages by checking if the logical page number associated with this physical page number is consistent with the mapping in FTL. It periodically chooses the blocks that has the most trimmed pages.</li> <li>GC reads the live pages out from the block</li> <li>write out those live pages to the end of the log and update FTL mapping</li> <li>erase the entire block</li> </ul>"},{"location":"csc369/#13-optimized-write-with-block-based","title":"1.3 optimized write with block-based","text":"<ul> <li> <p>write a page(optimized approach):</p> </li> <li> <p>Prepare an FTL mapping from logical block number to physical block number instead</p> </li> <li> <p>Given a logical page number, we calculate its logical block number(i.e: logical page number / number of pages in a block) and check its mapping.</p> <p>if not in mapping:</p> <ol> <li>allocate a new physical block at the end of the log(if this block has not been erased, erase it)</li> <li>write to the first page in the block</li> <li>add the logical block number, physical block number to mapping</li> </ol> </li> <li> <p>else:</p> <ol> <li>allocate a new physical block at the end of the log</li> <li>write the updated page to the page of the new block</li> <li>migrate other valid pages in the old block to the new pages</li> <li>update the logical block number to point to this physical block number</li> </ol> </li> </ul> <p>pro:</p> <ul> <li>we now use mapping to map block number to block number, which reduces the memory size ofFTL mapping</li> </ul> <p>cons:</p> <ul> <li>when we update an existing physical page, we need to migrate other valid pages in the same physical block to the new block in order to make their logical block number point to the new physical block in mappin. This increase the write amplifications to the naive approach, which is really bad.</li> </ul>"},{"location":"csc369/#14-optimized-write-with-hybrid-mapping","title":"1.4 optimized write with hybrid mapping","text":"<ul> <li> <p>write a page(optimized approach, complicated assumed not to be on final):</p> </li> <li> <p>Prepare 2 FTL mappings:</p> <p>one is a small set of per-page mappings in what we\u2019ll call the log table</p> <p>the other one is a larger set of per-block mappings in the data table</p> </li> <li> <p>Given a logical page number, we calculate its logical block number(i.e: virtual address / number of pages in a block)</p> </li> <li> <p>we then check log table to see if it has logical page number and data table to see if it has logical block number.</p> <ol> <li>if not in log table and not in data table:<ol> <li>allocate new physical page at the end of the log</li> <li>write content to the new page</li> <li>add the logical block, physical block to data table</li> </ol> </li> <li>if not in log table but in data table:<ol> <li>calculate its offset in block</li> <li>check if it is occupied. If so:</li> <li>allocate new physical page at the end of the log</li> <li>write content to new page</li> <li>add the logical page number, physical page number to log table</li> </ol> </li> <li>if in </li> </ol> </li> </ul> <p>What is over-provisioning:</p> <p>FTL makes a logical page space that is smaller than the physical page space. By keeping extra, \u201chidden\u201d pages around, the FTL tries to defer GC to a background task (thus removing GC from critical path of a write)</p> <p>Drawback issues of SSD:</p> <ul> <li>wear out</li> <li>High complexity of write to ssd due to erase operation</li> <li>program disturbance</li> </ul> <p>How to boost performance of SSD(performance goal):</p> <ul> <li>reduce write amplication</li> <li>wear leveling</li> <li>squential-programing to minimize program disturbance</li> </ul> <p>2 main costs of this organization:</p> <ul> <li>garbage collection(GC)</li> <li>Mapping table size</li> </ul>"},{"location":"csc369/#reference","title":"*. Reference","text":"<p>csc369: https://q.utoronto.ca/courses/250638</p> <p>csc469: https://q.utoronto.ca/courses/234302/modules</p> <p>https://www.cs.toronto.edu/dcs/ugdocs/course-outlines/2021/Fall/CSC469H1-Fall2021.pdf</p> <p>https://www.artsci.utoronto.ca/future/ready-apply/admission-requirements/second-degree-applicants</p> <p>https://www.artsci.utoronto.ca/current/academics/attendance-status</p> <p>https://internationalexperience.utoronto.ca/international-student-services/immigration/resources/connecting-with-isias/</p> <p>https://www.artsci.utoronto.ca/faculty-registrar/fees</p> <p>https://studentaccount.utoronto.ca/wp-content/uploads/2023/07/23-24-FAS-Woodsworth-INT_.pdf</p>"},{"location":"csc443/","title":"CSC443","text":""},{"location":"csc443/#week-2","title":"week 2","text":"<ul> <li> <p>volatility: data stay when power is off</p> </li> <li> <p>bytes addresable: 64-128 B</p> </li> <li>block addressable: 4-16 KB</li> </ul>"},{"location":"csc443/#1-disk","title":"1. disk","text":"<p>seek time - Move arms to position head on track (1-10 ms)</p> <p>rotational delay - wait for sector to rotate under head (0-5 ms)</p> <p>transfer time - moving data to/from disk surface (&gt; 0.01 ms)</p> <p>seek/rotational delays dominate</p> <p>disk layout principle(how to be fast):</p> <ul> <li>Large sequential reads/writes of adjacent sectors and tracks are fast</li> </ul> <p>soln to be large sequential reads/write:</p> <ul> <li>defragmentation</li> </ul>"},{"location":"csc443/#2-ssd","title":"2. SSD","text":"<p>SSD is divided into erase units(block), each erase unit is divided into page as screenshot above</p> <ul> <li>A page is the minimum read/write unit with typically 4kB, erase units typically 128kB or 256kB</li> <li>Pages must be written sequentially in an erase unit</li> <li>All data in an erase unit is erased at the same time</li> <li>Each erase unit has a lifetime (1-10K erases)</li> <li>within the erase unit, pages are written sequentially</li> </ul> <p>How pages are written and update in SSD:</p> <p>As the above screenshot shows, each erasing unit has multiple pages. Each page has 3 states: invalid, valid and free. The change of state of page requires different basic operations. Initially, the page is invalid and cannot be written. It can only be changed to free state through erase operation, then the page can be written. However, erase operation cannot erase one single page and must erase one entire erase unit. After the page becomes free, data can be written to the page and page state becomes valid. If we want to udate the data on that page, SSD will write new data on new page and the data on old page becomes stale. SSD will mark the old page from valid state to invalid state. Overall,  the meaning of page state:</p> <ul> <li>free: this page is empty and ready to write</li> <li>invalid: the data on the page is stale and is ready to be freed in GC</li> <li>valid: the data on the page is up to date and cannot be written</li> </ul> <p>Garbage Collection:</p> <p>As updates take place, more physical pages within the SSD get marked as invalid. Eventually, as the SSD runs out of free space, we must reclaim space taken up by invalid pages. The garbage collector picks the erase unit with the highest number of invalid pages. It migrates any remaining valid pages into an erase unit with free space, and it then erases the target erase unit</p> <p>Over-provisioning:</p> <p>Overprovisioning means any SSD is assigned more physical space than the logical capacity it exposes to the user. In this way, when encountering a scenario where the SSD is filled to full capacity with invalid pages, we can use this extra space as a temporary workspace to manage scheduled valid page merges and then put it back to user space after we reclaim blocks filled with invalid pages. Over-provisioning also allows  more invalid pages in user space to accumulate before free space runs out. In this way,  for each erase unit, there is a higher proportion on average of invalid pages and we could migrate fewer valid pages and thus reduces write amplification</p>"},{"location":"csc443/#21-write-amplcation","title":"2.1 Write amplcation","text":"<p>Intuition:</p> <p>Since SSD has garbage collection at behind scene, for every page the application writes, we not only physically write that page and in addition migrate some pages to a new page to reclaim the page we are writing. To measure this extra work, we use WA. The formula of wa:</p> <p>WA = 1 + (# of valid page within the target erase unit) / (# of invalid page)</p> <p>General formula:</p> <p>let x be the fraction of valid page in the target erase unit GC selects and y be the total page number</p> <p>WA = 1 + \\(\\frac{xy}{(1-x)y}\\) = 1 + \\(\\frac{x}{1-x}\\)</p> <p>write-amp from the user\u2019s perspective:</p> <p>suppose that B data items fit into each page and that the user updates one random entry in each request,</p> <p>WA = B * (1 + \\(\\frac{x}{1-x}\\))</p> <p>Worst-Case Write-Amplification:</p> <p>e worst-case write-amplification occurs when every full erase unit in the SSD has the same number of invalid pages. In this case, the SSD is unable to pick an erase unit with especially few live pages left to migrate and can only copy to over-provisioning</p> <p>WA = B * (1 + \\(\\frac{\\frac{L}{P}}{1 - \\frac{L}{P}}\\))</p> <p>WA Under Uniformly Random Writes:</p> <p>Uniformly randomly distributed write-amplification = B * (1 + \\(\\frac{\\frac{L}{P}}{2*(1 - \\frac{L}{P})}\\))</p>"},{"location":"csc443/#22-7-principles-how-to-make-most-of-the-ssd","title":"2.2 7 principles how to make most of the SSD","text":"<ol> <li>avoid small updates: because small Updates are terrible - they force reading a whole page and rewriting it.</li> <li>Avoid Random Updates: as they lead to garbage-collection overheads</li> <li>Instead write sequentially, and update data written at the same time all at once</li> <li>data die in different order, causing differen patterns of unavailable pages in SSD and might cause random updates</li> <li>Prioritize Reads over Writes</li> <li>Reads/Writes should be page aligned:</li> <li>why: for ex, Reading a misaligned page triggers 2 flash reads Updating a misaligned page triggers 2 flash reads and 2 flash writes</li> <li>Asynchronous I/Os for the win</li> <li>Defragmentation on SSDs is a Bad Idea</li> <li>Random read I/Os are fast, so relocating parts of a file to be close does not significantly improve read speed. On the other hand, it contributes to write-amplification and consumes the device\u2019s lifetime.</li> <li>Artificial Over-Provisioning: The less data you store, the more space the SSD will have for performing efficient garbagecollection</li> <li>Over-Provisioning: the logical capacity of ssd is significantly smaller than the whole physical capacity</li> </ol>"},{"location":"csc443/#3-raid","title":"3. RAID","text":"<p>PROBLEM RAID addresses:</p> <ul> <li>Our database size exceeds one drive and we need more storage</li> <li>A drive fails, and we need to recover its data</li> <li>We want to overcome the limits of one storage device speed</li> </ul> <p>technical terms:</p> <ul> <li>data striping: If we want to store a piece of data in data striping, this piece of data is segmented into equal-size partitions distributed over multiple disks, assuming the partition size is a page size(4096 bytes)</li> <li>data mirroring: Clone the piece of data on multiple disks</li> <li>parity block: It is a piece of block that is obtained by computing the XOR product of the aligned block row. It can be used to recover a block on failed disk, more can be seen in RAID 4</li> </ul>"},{"location":"csc443/#31-raid-0","title":"3.1 RAID 0","text":"<p>What is RAID 0:</p> <p>RAID 0 is based on data striping, which is to divide each piece of data into multiple stripes and distribute over many disks. As the screenshot displays, if a piece of data size is 4 page size, then the data is split into block1, block2, block3 and block4 on 2 disks in pic. Then when the system wants to read or write that data, it can read or write block1 and block3 from disk1 and block2 and block4 on disk2 simultaneously from all the disks and join them together to reconstruct the entire data stream. However, if one of the disks fails, the entire data becomes corrupt and worthless since it cannot be recreated anymore</p> <p></p> <p>benefits:</p> <ol> <li> <p>Much faster sequential writes and reads</p> </li> <li> <p>Also improvement for random writes and reads due to load balancing</p> </li> </ol> <p>drawback(reason not practical):</p> <p>No redundancy. If one disk fails, we lose data</p>"},{"location":"csc443/#32-raid-1","title":"3.2 RAID 1","text":"<p>What is RAID 1:</p> <p>RAID 1 uses the concept of data mirroring. Suppose the data size is 2 blocks in size, then the data is split into block1 and block2 and stored on both drives in pic. A multi-threaded process can access Block 1 from Disk 1 and Block 2 from Disk 2 at once as RAID 0. However, when it writes the data, it must write the 2 blocks on disk1 and also on disk2</p> <p></p> <p>benefits:</p> <ol> <li>Slower writes as they must make 2 copies </li> <li>Faster reads as we have a choice to read from a non-busy drive</li> </ol> <p>drawback:</p> <ol> <li>Allows recovery of a disk but costs 50% of storage capacity</li> </ol>"},{"location":"csc443/#33-raid-01raid10","title":"3.3 RAID 0+1(RAID10)","text":"<p>What is RAID 0+1</p> <p>In this setup, multiple RAID 1 blocks are connected with each other to make it like RAID 0.</p> <p></p>"},{"location":"csc443/#34-raid-4","title":"3.4 RAID 4","text":"<p>Goal: exploiting more than 50% if the storage capacity while allowing recovery</p> <p>What is RAID 4:</p> <p>RAID 4 stripes the data across multiple disks. Say we have 1 piece of data 1 with 6 pages in size. The first 3 blocks of the data 1 is split into block A1, A2 and A3 and and the last 3 blocks are stored in block B1, B2 and B3 in the 3 disks as the screenshot. In addition, we also have dedicated disk 4 to store parity block Ap, which is computed as Ap = A1 \\(\\oplus\\) A2 \\(\\oplus\\) A3 and Bp = B1 \\(\\oplus\\) B2 \\(\\oplus\\) B3 after writes to A1, A2, A3 and B1, B2, B3 happen. When one disk fails, say disk 2 fails, we lose A2 and B2 block of the data 1. We recover these two blocks by computing A2 = A1 \\(\\oplus\\)A3 \\(\\oplus\\) Ap and B2 = B1 \\(\\oplus\\) B3 \\(\\oplus\\) Bp. When data 2 is stored on block C and D of the 4 disks, similarly process goes as data 1. In this way, we avoid data cloning. However, because all the parity information is written on a single disk and disk 4 bears most load which is a bottleneck, the write performance is low</p> <p></p> <ul> <li> <p>how RAID 4 recover data: the input is the data stripe</p> </li> <li> <p>limits: </p> </li> <li>parity can only recover 1 input(not multiple inputs). When a disk fails, we know which disk fail, we use the parity and other good inputs to recover corrupted input </li> <li>the parity drive becomes bottle neck for random write</li> </ul> <p>how it handles Random Write: </p> <p>say we have a data with 3 pages in size and is splitted as block A1, A2 and A3, we want to update page A2:</p> <ol> <li>Simple soln:We updates page A2 to be A2', read other aligned pages(A1 &amp; A3), then we update Ap' = A1 \\(\\oplus\\) A2' \\(\\oplus\\) A3</li> </ol> <p>cost: Say we have N drives, the above process takes N-1 reads &amp; 2 write</p> <ol> <li>Real soln: We read A2 and Ap, updates A2 to be A2', then update Ap' = A2 \\(\\oplus\\) A2' \\(\\oplus\\) Ap</li> </ol> <p>costs: Say we have N drives, the above process takes 2 reads and 2 writes</p> <p>how it handles sequential write:</p>"},{"location":"csc443/#35-raid-5","title":"3.5 RAID 5","text":"<p>What is RAID 5:</p> <p>RAID 5 is very similar to RAID 4, but the parity info is distributed over all disks as Ap, Bp, Cp, Dp in screenshot. This has two benefits \u2014 First, there is no more a bottleneck as the parity stress evens out by using all the disks to store parity information. Second, there is no possibility of losing data redundancy since one disk does not store all the parity information.</p> <p></p> <p>how it handles sequential writes:</p> <ul> <li>sequential writes only have N/(N-1) overhead</li> </ul> <p>how it handles random writes:</p> <ul> <li>A random write requires 2 reads and 2 writes</li> <li>Random writes load is evenly distributed on all drives</li> </ul> <p>sequential read:</p> <ul> <li>Sequential Reads are as fast as possible</li> </ul> <p>random read:</p> <ul> <li>Random Reads have less flexibility than with mirroring</li> </ul> <p>limit:</p> <ul> <li>can only recover one disk</li> </ul> <p>Week 3</p>"},{"location":"csc443/#4-table","title":"4. table","text":""},{"location":"csc443/#41-how-to-store-tables-in-storage","title":"4.1 how to store tables in storage:","text":"<p>Problem:</p> <ul> <li>Reading/writing from storage at units of less than \u22484KB is not possible(reason: because within a disk or an SSD the minimum unit of reading or write is 4kB, disk takes time to rotate)</li> <li>Reading/writing at very large units consumes memory and is less flexible for applications(this is a problem in memory)</li> </ul> <p>Soln:</p> <p>To balance, we introduce page and I/O. An I/O (input/output) is one read or write request of one database page. Each table is composed of one or more pages(page size is at least 4kB or mutiples or 4kB)</p>"},{"location":"csc443/#42-how-table-keeps-track-of-each-page","title":"4.2 How table keeps track of each page:","text":""},{"location":"csc443/#421-linked-list","title":"4.2.1 linked list","text":"<p>This is the simplest soln, but there are limits:</p> <ul> <li>entails synchronous I/Os, which do not exploit SSD parallelism(parallelism due to multiple chips)</li> <li>small I/Os. because linked list does not gurantee pages allocated sequentially </li> </ul>"},{"location":"csc443/#422-directory","title":"4.2.2 Directory","text":"<p>soln to 1st limit: (Employ directory to allow reading many pages asynchronously.)</p> <p></p> <p>soln to 2nd limit: (Store multiple database pages contiguously along \u201cextents\u201d (8-64 pages))</p> <p>In a file system, an \"extent\" refers to a contiguous block of storage space used to store a file's data. Extents are an alternative to the traditional method of storing file data, which involves dividing it into smaller blocks or clusters scattered across the storage medium</p> <p>How to keep track of directories:</p> <ul> <li>use cataglog</li> </ul> <p>How to keep track of free pages/extent:</p> <ul> <li>linked list(slow soln)</li> </ul> <p></p> <ul> <li>bitmap(takes space)</li> </ul> <p></p>"},{"location":"csc443/#43-dam-model","title":"4.3 DAM model:","text":"<p>The DAM model helps us analze the cost of 4 db operations. It applies to disk storage</p> <p></p> <ul> <li>B is a parameter variable(vary from 1 to page size)</li> <li>assume within a schema, B is constant size</li> </ul> <p>Limits of DAM model:</p> <ul> <li>Ignores that sequential disk reads are more economical</li> <li>Ignores that SSD asynchronous I/O are faster</li> <li>Ignores SSD garbagecollection due to random writes</li> </ul>"},{"location":"csc443/#44-how-operations-implement","title":"4.4 how operations implement","text":"<p>N is number of entries in a table and B is number of entries stored in a page.</p>"},{"location":"csc443/#441-scan","title":"4.4.1 Scan","text":"<ul> <li>Inefficient way to store pages of db:</li> </ul> <p>This leads to N access of pages for each table, since each of the N entry is stored in a page</p> <ul> <li>Optimized way of storing pages of db:</li> </ul> <p></p> <p>The scan cost is O(N/B), the above inefficient soln is O(N)</p>"},{"location":"csc443/#442-delete","title":"4.4.2 Delete","text":"<ul> <li>soln: Scan of the table. Delete the row and creates a \u201chole\u201d in that page</li> </ul> <ul> <li>Cost: O(1) write and O(N/B) reads</li> </ul>"},{"location":"csc443/#443-updates","title":"4.4.3 Updates","text":"<ul> <li>soln: Scan and update. If newer version is too large, delete &amp; reinsert</li> <li>Cost: O(1) write and O(N/B) reads</li> </ul>"},{"location":"csc443/#444-insert","title":"4.4.4 Insert","text":"<p>Insert a row in the table</p> <ul> <li>soln #1: Scan &amp; find space</li> <li>cost: O(N/B) reads and O(1) write</li> <li>soln #2: We use a separate linked list to keep track of non-full pages and insert</li> <li>cost: <ul> <li>O(1) reads &amp; O(1) write for fixed-sized entries(No scan cost)</li> <li>O(N/B) reads &amp; O(1) write for variable-sized entries</li> </ul> </li> <li>soln #3: buffer insertions in memory until buffer full. Then a page fills up &amp; append to extent</li> <li>cost: No reads and O(1/B) of write</li> </ul>"},{"location":"csc443/#45-how-rows-are-stored-in-one-page","title":"4.5 How rows are stored in one page","text":"<ul> <li>assume row is fixed length: </li> </ul> <p>a db page can contain N rows and the page has the following 2 layouts( or solns):</p> <p></p> <ul> <li>assume each row is variable length in a page:</li> </ul> <p>place the bitmap at the start of page and real row at the end of the page:</p> <p></p>"},{"location":"csc443/#5-buffer","title":"5. Buffer","text":"<p>Why do we need buffer:</p> <p>To reduce read/write operations</p>"},{"location":"csc443/#50-buffer-management","title":"5.0 Buffer management","text":"<p>buffer pool data structure:</p> <p>Buffer pool Keep copies of hot pages in memory. Buffer pool is a hashtable. Consist of frames, each containing one page of data(e.g., 4 KB). Eventually it fills up. Must evict pages to clear space.</p> <p>Each frame must keep some metadata :</p> <ul> <li> <p>Pin count - How many users are currently using this page</p> </li> <li> <p>Dirty flag - indicates whether the page has been updated</p> </li> </ul> <p>What does buffer pool do:</p> <ul> <li>initially:</li> </ul> <p>Initialize all frames pin_count and dirty bit to be 0</p> <ul> <li>When a page is requested by a new user:</li> </ul> <ol> <li>Checks the buffer pool to see if some frame contains the requested page and, if so, increments the pin_count of that frame. If the page is not in the pool, the buffer manager brings it in as follows:  </li> <li>Hash the page to the location frame in the buffer pool. If the frame is free, choose that frame as the final location. If not, under the hash collision, find the final free frame slot. If there is no free frame in buffer pool:<ol> <li>Chooses a frame with pin_count = 0</li> <li>If there are many frames with pin_count = 0, use eviction policy to choose the page. If no page has 0 pin_count,  wait until some page is released before responding to the page request  </li> <li>After the page is selected, use the hash collision policy to hash the page to the selected page.</li> </ol> </li> <li>Increments the new page's pin_count  </li> <li>If the dirty bit for the replacement frame is on, writes the page it contains to disk (that is, the disk copy of the page is overwritten with the contents of the frame) . Otherwise, just evict the page and it is to drop that page without writing to the disk</li> <li>Reads the requested page into the replacement frame  </li> <li>Returns the (main memory) address of the frame containing the requested page to the requestor  </li> </ol> <ul> <li>After the page is requested by old user and the page is requested to release:</li> </ul> <ol> <li>decrement the pin_count to unpin</li> </ol> <ul> <li>After the page is requested and the page is requested to modify:</li> </ul> <ol> <li>decrement the pin_count to unpin</li> <li>set the dirty bit on</li> </ol>"},{"location":"csc443/#51-evicton-policy","title":"5.1 Evicton policy","text":""},{"location":"csc443/#511-random","title":"5.1.1 Random","text":"<p>Evict whichever page collides in the hash table with a new page</p> <ul> <li>pro: No additional metadata needed</li> <li>con: May evict a frequently used page</li> </ul>"},{"location":"csc443/#512-fifo","title":"5.1.2 FIFO","text":"<p>Evict Page that was inserted the longest time ago.</p> <p>Implementation from the slide: queue(or actually ring buffer):</p> <p>Cons:</p> <ul> <li> <p>pages we evict have different frames than pages we insert</p> </li> <li> <p>soln:</p> <ul> <li> <p>need a hash collision algorithm</p> </li> <li> <p>need  10%-20% capacity</p> </li> </ul> </li> <li> <p>Oldest page may still be frequently used. We\u2019ll try to do better</p> </li> </ul>"},{"location":"csc443/#513-lru","title":"5.1.3 LRU","text":"<p>Evict page that was used last the longest time ago</p> <p>Implementation: Doubly-linked list</p> <p>Cons:</p>"},{"location":"csc443/#514-clock","title":"5.1.4 Clock","text":"<p>Traverse hash table circularly as a clock. Set the page with 0 flag to be 1. Find the page with the first 0 flag and evict it.</p> <p>Pro:</p> <ol> <li>lower overheads as there is no queue</li> <li>bitmap takes little extra space</li> </ol> <p>Con:</p> <ol> <li>can evict \u201chotter\u201d pages than LRU, But still better than FIFO</li> </ol>"},{"location":"csc443/#week-4","title":"Week 4","text":""},{"location":"csc443/#6-index","title":"6. Index","text":"<p>Index is a way to efficiently optimized:</p> <pre><code>SELECT * FROM table t WHERE A = 'v';\n</code></pre> <p>To measure the performance, we introduce I/O cost and CPU cost:</p> <ul> <li>I/O cost: number of times to read/write a data page in order to find the target data page</li> <li>CPU cost: number of comparison to find the target data page and number of comparison to find the target key within the target data page</li> </ul>"},{"location":"csc443/#61-naive-way","title":"6.1 Naive way","text":"<p>How data are stored and how search occurs:</p> <p>The db keeps track of the page of each table. When db wants to find a key, db just access each page of the table</p> <p>Operation:</p> <ul> <li>insert:</li> <li>searchSingleEntry</li> <li>delete</li> <li>update:</li> </ul> <p>cost: </p> <ul> <li>worst case: O(N/B) I/Os</li> </ul>"},{"location":"csc443/#62-zone-map","title":"6.2 Zone map","text":"<p>How data is stored and how search occurs:</p> <p>The db keeps track of the page of each table. Each page stores two extra information: the max and min key within this page. This saves time when db wants to find a key within the page, but does not save time when db tries to find the target page. When db wants to find a key, db just access each page of the table</p> <p>Operation:</p> <ul> <li>insert:</li> <li>searchSingleEntry</li> <li>delete</li> <li>update:</li> </ul> <p>Pro:</p> <ul> <li>may be g</li> </ul> <p>cost: </p> <ul> <li>worst case: O(N/B) I/Os</li> </ul>"},{"location":"csc443/#63-sorted-file","title":"6.3 sorted file","text":"<p>How data is stored and how search occurs:</p> <p>We can view one column A of table as a continuous array when we join the rows of the pages of the table. For this method, we ensure the A column stored on each page of the table is sorted. Then when we want to find the data page where the target key lies in, we look at the first key of each page \\(k_1, k_2, ... k_N\\) and perform binary search. Ultimately, we find the target key k between \\(k_i\\) and \\(k_{i+1}\\) We can conclude the target key may potentially be at the page of \\(k_i\\) . Then we do binary search at the page of \\(k_i\\) to find k</p> <p>Operation:</p> <ul> <li>insert:</li> </ul> <p>Binary search to find proper location, and then push rest of elements up by one slot</p> <ul> <li>searchSingleEntry</li> </ul> <p>we do binary search on the first key of each page to determine where is the data page. Then in that page, we do binary search to find the target key.</p> <ul> <li> <p>delete</p> </li> <li> <p>update:</p> </li> </ul> <p>Performance:</p> operation worst case CPU worst case I/O insert/update O(N/B) search O(\\(log_2(N/B)\\)) + O(\\(log_2(B)\\)) O(\\(log_2(N/B)\\)) <p>Pro:</p> <ul> <li>searchSingleENtry is fast</li> </ul> <p>Con:</p> <ul> <li>limit: Can only sort based on one column, but what if we want to search across other columns</li> </ul>"},{"location":"csc443/#64-binary-tree","title":"6.4 Binary tree","text":"<p>How is data stored and how search occurs:</p> <ol> <li>We first use an augmented binary tree to store each key of the column A of the table at each node of the tree. Each node stores the key and its associated page number of the table in storage</li> </ol> <p></p> <ol> <li>We store each tree node in an array of tree nodes. The array needs to be written to the store. The content of the array may be in different page number of the sorage. Inside each element of the array, each element stores the key value, its associated data page number, its left child &amp; the left child's page(since each element of the array may be in different page of the storage), its right child &amp; the right child's page number. Now suppose each element of the tree array is in different page number and when we do tree traversal, we need to read \\(log_2(N)\\) pages because the first page of the array can store B nodes from top level to below. Thus, we have fewer pages to read, which is \\(log_2(B)\\). Therefore, the total I/O cost is \\(log_2(N) - log_2(B) = log_2(N/B)\\) </li> </ol> <p>Operation:</p> <ul> <li>insert:</li> <li>searchSingleEntry</li> <li>delete</li> <li>update:</li> </ul> <p>Performance:</p> operation worst case CPU worst case I/O search O(\\(log_2(N)\\)) O(\\(log_2(N/B)\\))"},{"location":"csc443/#65-hash-table","title":"6.5 Hash table","text":"<ul> <li> <p>We store a hash table in storage</p> </li> <li> <p>We use hash function to map key to a bucket and each bucket contains B entries mapping a key to its data page(hash collision policy)</p> </li> <li>We double number of buckets when hash table reaches capacity</li> </ul> <p>read/write cost:</p> <ul> <li>O(1) per random query and no collision happens</li> <li>collision happens</li> <li>double expand hashtable:</li> </ul> <p>downside:</p> <ul> <li>No support for range reads</li> <li>Expansion leads to performance slumps</li> <li>We waste 50% of capacity right after expansion</li> </ul>"},{"location":"csc443/#66-extendible-hashing","title":"6.6 Extendible hashing","text":"<p>how it works:</p> <ol> <li>we first have a small directory that comrises of small hash prefix to the data pages</li> <li>When we create data, we hash the data using the small directory. When collision happens, say we use chaining and we create one overflowing page</li> <li>We can then double directory to allow more mapping to data pages and finally we copy the overflowing page to the new data page from the doubled directory. </li> </ol>"},{"location":"csc443/#67-b-tree","title":"6.7 B-tree","text":"<p>Goal: </p> <p>we want to read at least B entries from storage at a time, let\u2019s make each tree node have B entries to prune the search space by a factor of B</p> <p>defn of B-tree:</p> <p>A B-tree T is a rooted tree(whose root is T.root) that has following property:</p> <ol> <li>Each node x has the following attributes:</li> <li>x.n, the number of keys in node x</li> <li>the (x.n) keys in the list: x.key<sub>1</sub>, x.key<sub>2</sub>, ... x.key<sub>x.n</sub>,  stored in non-decreasing order</li> <li>x.leaf, boolean value to indicate whether this node is leaf</li> <li>Each internal node x has (x.n + 1) children node</li> <li>Say x.c<sub>i</sub> is the pointer to the i<sup>th</sup> child node of node x. If k<sub>i</sub> is any key stored in the subtree with root x.c<sub>i</sub>, then k<sub>1</sub> \\(\\leq\\) x.key<sub>1</sub> \\(\\leq\\) k<sub>2</sub> \\(\\leq\\)  x.key<sub>2</sub> \\(\\leq\\) ... \\(\\leq\\) k.key<sub>x.n</sub> \\(\\leq\\) k<sub>x.n+1</sub> </li> <li>All leaves have the same depth  h</li> <li>let \\(t \\geq 2\\) be the min degree of B-tree</li> <li>Every node other than root has at least (t - 1) keys, at least t children pointer</li> <li>Every node contains at most (2t - 1) keys, at most 2t children pointer</li> </ol> <p>Notation:</p> <ul> <li>we use B= 2t-1 to represent the number of keys in a full leaf node in B-tree. Each leaf occupies a whole page size on the disk as default</li> </ul> <p>Example of structuring B - tree:</p> <p></p> <ol> <li>Suppose we have a list of keys and we sort them. We group of the keys with each group size of 3. Each group is the tree node of B-tree. In the leaf node, there stores the mapping from the key to the page number in the diagram above</li> <li>In the upper level, we store the max key of each node. In the left upper node, we store 7, 22 as the max key of the left 2 leaf node. In the right, we store the max key of the right 3 leaf nodes which are 35, 73, 90</li> <li>The root node stores 31 which is the max key of the 3rd leaf node</li> </ol> <p>cost:</p> <p>let N be the number of keys in total, B be the max number of keys in a b-tree node:</p> <ul> <li> <p>depth of b-tree: \\(log_B(N)\\) ,</p> </li> <li> <p>search cost(I/O cost): O(\\(log_B(N)\\))</p> </li> <li> <p>CPU cost: O(\\(log_B(N) * log_2B\\)): we searched log_b(B) and for each node, we searched among the B keys </p> </li> </ul> <p>how to insert(To be filled):</p> <ol> <li>find target node</li> <li>if node is full(node key # == B):</li> <li>split target node into 2 new half-full nodes</li> <li>add key sto the new node</li> <li>compare the new key with its parent node keys and propagate split upward potentially</li> </ol>"},{"location":"csc443/#week-5","title":"Week 5","text":""},{"location":"csc443/#7-lsm-tree","title":"7. LSM tree","text":"<p>type:</p> <ul> <li>Leveled LSM-tree</li> <li>Basic LSM-tree</li> <li>Tiered LSM-tree</li> </ul>"},{"location":"csc443/#71-basic-lsm-tree","title":"7.1 Basic LSM tree","text":"<p>Insert:</p> <ol> <li>insert data at the buffer at level 0. Once filled, we sort the key at buffer &amp; flush buffer and place buffer at level 1</li> <li>We then insert data at buffer at level 0. Once buffer at level 0 filled up. We mergesort the old buffer at level 1 and current filled buffer at level 1, then place the new sorted buffer at level 2. </li> <li>We delete buffer at level 0 and level 1. </li> <li>We repreated this until level N</li> </ol> <p>Get:(total runTime: O(log2 N/P) * O(log2 N/B))</p> <ol> <li>searches the LSM-tree from smaller to larger levels  (O(log2 N/P), P is the length of SST at each level)</li> <li>search each level                                                              (O(log2 N/B), the Nth level SST length is log2(N/B))</li> <li>stop when they find the first matching entry</li> </ol> <p>Scan: Return most recent version of each entry in the range across entire tree</p> <p>(O(log2(N/B) * logB(N) + S/B))</p> <ol> <li>Allocate an in-memory buffer (&gt;1 page) for each level</li> <li>Search for start of key range at each level </li> <li>Initialize counter to smallest key in range</li> <li>Loop:</li> <li>Bring youngest matching entry to output </li> <li>Increment counter</li> </ol> <p>Since there are copies during compaction of write:</p> <ul> <li>number of times each entry is copied: O(log2 N/B)</li> <li>price of each copy: O(1/B) reads &amp; writes</li> <li>total cost: O((log2 N/B)/B) read &amp; write I/Os</li> </ul>"},{"location":"csc443/#72-leveled-lsm-tree","title":"7.2 Leveled LSM tree","text":"<p>Different from basic LSM tree, the ratio between length of level(n+1) and level n is T instead of 2</p> <p>Performance:</p> <ul> <li>Lookup cost(O(\\(log_T(N/P)\\)))</li> <li>Insert cost: O(T/B * \\(log_T(N/P)\\))</li> </ul>"},{"location":"csc443/#73-tier-lsm-tree","title":"7.3 Tier LSM tree","text":"<p>Performance:</p> <ul> <li>lookup cost: O(T * \\(log_T(N/P)\\))</li> <li>insert cost: O(1/B * \\(log_T(N/P)\\))</li> </ul>"},{"location":"csc443/#week-6","title":"Week 6","text":"<p>Recall:</p> <p>B-tree random write leads to large write aplication. Thus, we use LSM tree</p> <p>level of LSM tree:</p> <p>\\(L = log_T(N/P)\\) , N is the number of entries, P is the size of memtable, T is the resize ratio</p>"},{"location":"csc443/#8-bloom-filter","title":"8. Bloom filter","text":"<p>Probility tool to tell whether the key is in the set. It's a bit vector with k hash functions.</p> <p>operation:</p> <ul> <li>insert: hash the key and outcome is the index in the bitmap. We then set the bit on that index to be 1.</li> <li>query: If all bits are set to 1, it tells the key is probably in the set. If there exists bit that is 0, it tells the key is not in the set</li> </ul> <p>However, there's a small probability of false positives, where the Bloom filter incorrectly suggests that an element is in the set when it's not. There are no false negatives; if the filter says the element is not in the set, it's definitely not.</p> <ul> <li> <p>true positive: check if all bits are 1</p> </li> <li> <p>true negative: check if there exists bit that is 0</p> </li> <li> <p>false positive\uff1a bloom filter tells it is in but it is not in</p> </li> <li> <p>false negative: bloom filter tells it is not in but it is in</p> </li> </ul> <p>if it is in , then bloom filter returns positive</p> <p>Key problem:</p> <ul> <li>how many hash functions to use?</li> <li>how to choose K</li> <li>Operation cost</li> </ul>"},{"location":"csc443/#81-how-many-hash-functions-to-use","title":"8.1 how many hash functions to use:","text":"<p>too few hash functions lead to high false positive rate. But too many hash functions wind up increasing the FP</p> <p></p> <p>Suppose insert N keys in bloom filter, we have bloom filter with K bits in length,</p> <p>M = K / N(K &gt; N), how to choose K?</p> <p>Optimal # hash functions = ln(2) \u00b7 M(M is the number of bits per entry, typically 10)</p> <p>assuming the optimal # hash functions, 2<sup>\u2212M\u22c5ln(2)</sup> false positive rate</p> <p>In reality, we first have ideal false positive rate, then we deduct M, then we deduct Optimal # hash functions </p>"},{"location":"csc443/#82-operation-cost","title":"8.2 Operation cost","text":"operation cost Insert M * In(2) Positive query M * ln(2) Avg. Negative Query 2 false positive rate \\(2^{\u2212M\u22c5ln(2)}\\) <p>Cost for bloom filter searching key in LSM - tree:</p> <ul> <li>worste case: O(M  * L)</li> <li>Avg worst case: O(M + L)</li> </ul>"},{"location":"csc443/#9-monkey","title":"9. Monkey","text":""},{"location":"csc443/#10-dstoeskv","title":"10. DSTOESKV","text":"<p>design:</p> <p>At higher level, we use tier LSM tree mechanism. At the last level, we use level LSM tree mechanism.</p> <p>reads co</p>"},{"location":"csc443/#week-7","title":"Week 7","text":""},{"location":"csc443/#quick-sort","title":"Quick sort","text":"<p>benefit:</p> <ul> <li>In-place algorithm: no need for x2 space like merge-sort</li> <li>Sequential memory access is fast</li> </ul>"},{"location":"csc443/#b-tree-sort","title":"B-tree sort","text":"<ul> <li>If internal nodes are in memory: O(N) reads &amp; writes</li> <li>If internal nodes are in storage:  O(N logB N) reads &amp; O(N) writes</li> </ul>"},{"location":"csc443/#multi-way-merge-sort","title":"Multi-way Merge-Sort","text":"<ol> <li>Sequentially read chunks that fit in memory, sort, and store back as temporary files</li> <li>Allocate a buffer for each input file and merge into output stream</li> </ol> <p>limit:</p> <ul> <li> <p>lots of write amplications</p> </li> <li> <p>each sorted temporary file double the storage</p> </li> <li> <p>We have little memory as a buffer to store all of the sorted file. Thus, we merge at least 2 at a time</p> </li> </ul>"},{"location":"csc443/#midterm-review","title":"Midterm review","text":""},{"location":"csc443/#1-b-tree","title":"1. B-tree","text":""},{"location":"csc443/#2-lsm-tree","title":"2. LSM tree","text":""},{"location":"csc443/#summary","title":"*. Summary","text":"<p>Notation:</p> Term Description Unit N Total number of entries entries L Number of levels levels B Number of (key, value) entries that fit into a disk page entries K Number of (key, pointer) entries that fit into a disk page entries E Size of an entry bits P Size of the buffer in disk pages pages P<sub>1</sub> number of entries in a buffer, P<sub>1</sub> = P * B entries S number of entries in range scan entries T Size ratio between adjacent levels T<sub>lim</sub> Size ratio value at which point L converges to 1 M Total amount of main memory in system bit M<sub>buffers</sub> Main memory allocated to the buffer bit M<sub>filters</sub> Main memory allocated to the Bloom filters bit M<sub>pointers</sub> Main memory allocated to the fence pointers bit <p>RunTime summary:</p> data structure insert update search scan B+tree(in storage, unclustered) I/O: O(log<sub>B</sub>N) +O(1) I/O: O(log<sub>B</sub>N)CPU: O(log<sub>B</sub>N * log<sub>2</sub>B) = O(log<sub>2</sub>N) I/O: O(log<sub>B</sub>N + S) B+tree(internal in memory) I/O: O(1) + O(1) + GC I/O: O(1) B+tree(clustered) I/O: O(log<sub>B</sub>N + S/B) basicLSM tree(without B-tree) I/O: O(log2(N/P<sub>1</sub>) * log2(N/B)) basic LSM(with B-tree in storage) I/O: O(log<sub>2</sub>N/P<sub>1</sub> * 1/B) I/O: O(log2(N/P<sub>1</sub>) * logB(N)) I/O: O(log2(N/P) * logB(N) + S/B) clustered basic LSM(with B-tree in memory) I/O: O(log2(N/P<sub>1</sub>) ) unclustered basic LSM(with B-tree in memory) I/O: O(1/B + L<sub>U</sub>/K) I/O: L<sub>U</sub> + 1 I/O: O(L<sub>U</sub> + S) level LSM tree I/O: O(T/B logT (N/P)) I/O: O(logT(N/P)) tier lsm tree I/O: O(1/B logT (N/P)) I/O: O(T* logT(N/P)) Append only table I/O: O(1/B) I/O: O(N/B) sorted table zone map I/O: O(N/B) (worst)  sorted file I/O: O(N/B) I/O: O(log<sub>2</sub>(N/B))(worst) CPU: O(log<sub>2</sub>(N/B) + log<sub>2</sub>B) = O(log<sub>2</sub>N) binary tree I/O: O(log<sub>2</sub>(N/B))CPU: O(log<sub>2</sub>(N)) hashtable(in storage) <p>I/O runtime: (L = log<sub>T</sub>(N/P<sub>1</sub>) )</p> data structure operation I/O Runtime append-only table query read O(N/B) insert read 0 write O(1/B) Basic LSM(with B-tree not in memory) query read O(log<sub>2</sub>(N/P<sub>1</sub>) *log<sub>B</sub>(N)) insert read O(log<sub>2</sub>(N/P<sub>1</sub>) * 1/B) write O(log<sub>2</sub>(N/P<sub>1</sub>) * 1/B) Basic LSM(with B-tree in memory) query read O(log<sub>2</sub> (N/P<sub>1</sub>)) insert read O(log<sub>2</sub>(N/P<sub>1</sub>) * 1/B) write O(log<sub>2</sub>(N/P<sub>1</sub>) * 1/B) B-tree(not in memory) query read O(log<sub>B</sub>N) insert read O(log<sub>B</sub>N) write O(1) + GC B-tree(in memory) query read O(1) insert read O(1) write O(1) + GC Level LSM query read O(log<sub>T</sub>(N/P<sub>1</sub>)) insert read O(log<sub>T</sub>(N/P<sub>1</sub>) * T * 1/B) write O(log<sub>T</sub>(N/P<sub>1</sub>) * T * 1/B) Tier LSM query read O(O(log<sub>T</sub>(N/P<sub>1</sub>) * T ) insert read O(log<sub>T</sub>(N/P<sub>1</sub>) * 1/B) write O(log<sub>T</sub>(N/P<sub>1</sub>) * 1/B) LSM with bloom filter uniform FPR across each level Monkey Dotevski <p>B-tree:</p> <ul> <li>clustered vs unclustered</li> <li>cluster height: log<sub>K</sub>(N / K), number of leaf node: N/K</li> <li> <p>unclustered height: log<sub>B</sub>(N/B), number of leaves: N/B</p> </li> <li> <p>internal node in buffer:</p> </li> <li>in buffer</li> <li> <p>in storage</p> </li> <li> <p>Each insert costs 1 read and 1 write I/O(because of split node) and each get costs 1 I/O</p> </li> <li>in storage:</li> <li>insert/search/update:   O(log<sub>B</sub>N) Read I/O  + O(1) Write I/O</li> <li>internal node in memory buffer:</li> <li>O(N/B) keys are stored in memory</li> </ul> <p>lsm tree:</p> <ul> <li>height: L = \\(\\lceil log_T(\\frac{NE}{M_{buffer}} \\cdot \\frac{T-1}{T}) \\rceil\\) = O(log<sub>T</sub> N/P<sub>1</sub>), T<sub>lim</sub> = \\(\\frac{NE}{M_{buffer}}\\)= N/P<sub>1</sub> </li> <li>Each insert costs 1/B read and 1/B write I/O and each get costs 1 I/O.</li> <li>operation worst case</li> <li> <p>get: find a key that does not exist in memtable and sst files(in this way, the )</p> </li> <li> <p>each file has B-tree</p> </li> <li>yes<ul> <li>B-tree in memory</li> <li>B-tree is not in memory</li> </ul> </li> <li>no</li> <li>merge policy:</li> <li>tier:<ul> <li>With tiering there are at most T \u2212 1 runs at every level (when the T th run arrives from the previous level it triggers a merge operation and a push to the next level)</li> </ul> </li> <li> <p>level</p> </li> <li> <p>clustered vs unclustered</p> </li> </ul> <p>Bloom filter:</p> <ul> <li>Let M be the number of bits per entry(common M value is 10, which gives 1 false positive rate)</li> <li>optimal #of hash func: ln(2) * M</li> <li>false positive rate: 2<sup>-M*ln(2)</sup></li> <li>Operation cost(in memory):</li> <li>Insertion: each insertion takes M * ln(2)</li> <li>postive query: M * ln(2)</li> <li>Avg negative query: 1 + 1/2 + 1/4 + ... = 2</li> <li>get cost:<ul> <li>total worst case: O(M * L)  (the key is not in LSM tree and we get false positive query all the way to the L level of the tree)</li> <li>Avg worst case: O(M + L)</li> </ul> </li> <li>LSM tree integrated with bloom filter I/O cost:</li> <li>tier:<ul> <li>get: O(2<sup>-M</sup> * L * T)</li> </ul> </li> <li>level:<ul> <li>get: O(2<sup>-M</sup> * L)</li> </ul> </li> </ul> <p>space amplication:</p> <p>let unq be the number of unique entries,</p> <p>SA = \\(\\frac{N}{unq} - 1\\)</p> <p>worst case SA:</p> <ul> <li>leveling:  With leveling, the worst-case space-amplification occurs when entries at Levels 1 to L \u2212 1 are all updates to different entries at Level L thereby rendering at most a fraction of 1 T entries at level L obsolete. O(\\(\\frac{1}{T}\\))</li> <li>tiering:  the worst-case occurs when entries at Levels 1 to L \u2212 1 are all updates to different entries at Level L, and where every run at Level L contains the same set of entries (O(T))</li> </ul> <p>Monkey design:</p> <ul> <li>how does Monkey allocate memory optimally across Bloom filters to minimize lookup cost?</li> </ul> <p>monkey  finds the optimal number of levels Lfiltered to which Bloom filters should be allocated and lets the optimal FPR at Level i is T times higher than the optimal FPR at Level i \u2212 1. Thus, the lower level of lsm tree in monkey has more bits per entry in bloom filter and the deeper level has fewer bits till in some levels there is no bit. The intuition is that the amount of main memory needed for achieving a low FPR at deeper levels is significantly higher since they have exponentially more entries and the worst case runtime of false positive query is O(ML) and the average worst case is O(M + L), where M is the number of bits per entry and L is the number of levels in lsm tree.</p> <ul> <li> <p>bloom filter worst case: O(M * L + L<sup>2</sup>), Avg worst case: O(M + L)</p> </li> <li> <p>I/O cost with monkey design:</p> </li> <li> <p>level:</p> <ul> <li>(2<sup>-M</sup> /T<sup>L-1</sup> +... +2<sup>-M</sup> / T<sup>2</sup> + 2<sup>-M</sup>/ T + 2<sup>-M</sup> /T<sup>0</sup>  +) = O(2<sup>-M</sup>), while get i/o cost for regular design is </li> </ul> <p>O(2<sup>-M</sup> * L)</p> </li> </ul> <p>Dostoevski:</p> <ul> <li>What is lazy leveling:</li> </ul> <p>: it applies leveling at the largest level and tiering at all other levels. As a result, the number of runs at the largest level is 1 and the number of runs at all other levels is at most T \u2212 1 </p> <ul> <li>trade-off:</li> </ul> <p>improves the cost complexity of updates, (2) maintains the same complexity for point lookups, long range lookups, and space-amplification, and, even though it trades off the point lookup (3) provides competitive performance for short range lookups</p> <ul> <li> <p>I/O cost:</p> </li> <li> <p>get: O(2<sup>-M</sup>)</p> </li> <li>insert: O(1/B) + ... + O(1/B) + O(T/B) = O((L+T)/B)</li> </ul> <p>multi-way merge sort algorithm:</p> Notation description units M number of entries in buffer entries <p>Let buffer size be M entries, data size be N entries, each page contains B entries. The algorithm uses the buffer to sort the data entries:</p> <p>step1: partition the N entries into (N/M) partitions with each partition's size M entries</p> <p>step2: Use the buffer to sort each partition so that each partition is sorted(sort each partition needs O(Mlog<sub>2</sub>M) and (N/M) partitions in total takes O(Nlog<sub>2</sub>M))</p> <p>step3: Since we have buffer size M entries and we have M/B buffer, then we select M/B partition from the N/M partitions and assign one buffer from the M/B buffer to each of the selected partition. For each partition, we copy one page from the partition to their corresponding buffer. Since each buffer contains the sorted page from the sorted partition, we choose the minimum element from the first key of each of the buffer, put it in the array which will be the merged partition of these partitions, and pop the key. The second key of that buffer will be compared with the rest of the buffer's first key. This process continues until there is no key in one of the buffer. Then we read the next page from the corresponding partition of that buffer. This continues until all partitions are read and we obtain this merged partition called P1</p> <p>step4:  We then select the next M/B partions from the N/M partition for the second pass and does the same process as step 3 and obtain another mered partition called P2</p> <p>step5: After some passes, we now select M/B partition from those merged partition and does the same process as step3 until there is only 1 merged partition. This takes log<sub>M/B</sub>(N/M) passes</p> <ul> <li> </li> </ul> <p>log<sub>M/B</sub>(N/M) + 1 = log<sub>M/B</sub>(N/B),   precisely \\(log_{M/B-1}(N/M) + 1\\)</p> <p>(if we cannot allocate a buffer for each of the (N/M) partition, then this is the number of passes to merge N/M partitions into one and each pass we merge M/B partitions)</p> <ul> <li> <p>M = \\(\\sqrt {N \\cdot B}\\) (number of entries in buffer if we can allocate one buffer for each (N/M) partition and the number of pass is 1)</p> </li> <li> <p>Given the above M, we have \\(\\sqrt{\\frac{N}{B}}\\) partitions</p> </li> <li> <p>CPU &amp; I/O cost:</p> </li> </ul> Operation CPU cost I/O cost multi-way merge sort O(N \\(\\cdot\\) log<sub>2</sub>N) O(N/B * log<sub>M/B</sub>(N/B)) (partition) O(M \\(\\cdot\\) log<sub>2</sub>M * N/M) = O(N\\(\\cdot\\) log<sub>2</sub>M) O(N/B) (merging: traverse buffer) O(\\(\\sqrt{\\frac{N}{B}} \\cdot N\\)) N/B * \\(\\lceil log_{M/B} (N/M) \\rceil\\) (merging: min heap) O(\\(log_2(\\sqrt {\\frac{N}{B}} \\cdot N)\\)) N/B * \\(\\lceil log_{M/B} (N/M) \\rceil\\) 2-pass merge sort(given M = \\(\\sqrt{N \\cdot B}\\)) O(N\\(\\cdot\\) log<sub>2</sub>M) O(N/B) <p>Buffer pool:</p> <ul> <li>random eviction:</li> <li>Evict whichever page collides in the hash table with a new page</li> <li>pro: Simple, CPU-efficient, no extra metadata</li> <li>limit: May evict a frequently used page</li> <li>FIFO:</li> <li>steps:<ol> <li>Use a queue to maintain the order of the page that is inserted</li> <li>If a page is inserted, we add to the rear of the queue; if we want to evict, we pop the head</li> </ol> </li> <li>limit: Unlike the random policy, pages we evict now have different frames than pages we insert. We then need hash collision resolution algo and extra capcity</li> <li>LRU:</li> <li>steps(Evict page that was used last the longest time ago)<ol> <li>Use a queue to maintain the order of the page that is inserted</li> <li>when we insert a page, we add to the rear</li> <li>when we evict a page, we pop the head</li> <li>when we access a page, we move the page from its location in queue to rear</li> </ol> </li> <li>limit: CPU overhead</li> </ul> <p>trade-off:</p> <ul> <li>get query</li> <li>scan</li> <li>insert</li> <li>memory cost</li> </ul> <p>log rules:</p> <ul> <li>log<sub>a</sub>(b) = \\(\\frac{log_c(b)}{log_c(a)}\\)</li> <li>log<sub>a</sub>b * log<sub>c</sub> a = log<sub>c</sub> b</li> </ul>"},{"location":"csc443/#iterations","title":"iterations:","text":""},{"location":"csc443/#week-9","title":"Week 9","text":""},{"location":"csc443/#1-circular-log","title":"1. Circular log","text":"<p>also known as \"Index+Log\", \"Circular Log\", \"log-structured hash table\", \"log structured file system\"</p> <p>Components:</p> <ul> <li>buffer: an in-memory buffer used to buffer insertion and then flush to the disk</li> <li>Index: an in-memory hashtable that maps the key to the location in log(storage). Inside the index, the data is of the form  <li>live bytes: an in-memory hashtable that maps the blockId to an integer counter that represents how many live bytes are in this block. The GC will use this data to determine which block to be recycle. The data inside live bytes is  <li>Log: consists a set of blocks(where block size &gt;= buffer size) on the disk. Inside each block, the data is of the form  <p>Soln 1:</p> <ul> <li> <p>insert:</p> </li> <li> <p>insert the  into the buffer <li> <p>if the buffer is full:</p> <ol> <li>flush the buffer to the end of the log</li> <li>add all keys from the buffer to the index and set their values as the pointers to the end of the log</li> </ol> </li> <li> <p>delete(version 1):</p> </li> <li> <p>read the entry from log on disk through its blockId in index to check the value's size</p> </li> <li>subtract its size from the blockId's counter in live-bytes section</li> <li> <p>delete the  from the index. That's all. The actual  pair will be deleted in gc <li> <p>delete(version 2):</p> </li> <li> <p>In order to get rid of the read I/O from the delete version 1. We can store an extra value of size that represents the size of the value associated with the key in index as the following screenshot. Then we can retrieve its size by just looking through its key in index</p> <p></p> </li> <li> <p>same as version 1</p> </li> <li> <p>same as version 1</p> </li> <li> <p>get:</p> </li> <li> <p>Check if the buffer has the  pair <li> <p>if not, check the index for the block and read the data from the log through the block Id</p> </li> <li> <p>update:</p> </li> <li> <p>call delete</p> </li> <li> <p>call insert</p> </li> <li> <p>GC:</p> </li> <li> <p>Periodically check if the global threshold is reached(live data L /physical space P). If so, trigger the following steps for GC</p> </li> <li> <p>Pick the blockId with the least live data left from live-bytes section</p> </li> <li> <p>Read and scan the block through the blockId.</p> <p>For each key value pair entry in this block</p> <ol> <li>if the key is not in the hashtable in index section, it means the key is deleted and continue</li> <li>if the key is in index section but its blockId is not this blockId, continue</li> <li>if the key is in index section but its blockId is this blockId, migrate this key/value entry to the last block if the last block not full and update index blockId for these keys(O.W: just allocate a new block and point to the new block)</li> </ol> </li>"},{"location":"csc443/#2-hotcold-separation","title":"2. hot/cold separation","text":"<p>This is a problem</p> <p>We call the page entries that are frequently updated hot pages, while the page entries that are seldom updated cold pages. THE cold data gets mixed with more hot data within an erase unit. Since hot entries are invalidated quickly, by the time we garbage-collect, we only migrate cold data which does not actually need to be migrated and thus increase the WA. This may lead to the WA worst case</p> <p>The soln to the above problem can be separate cold and hot data into 2 different areas. Here is 2 implementations:</p> <p>soln 2(hot and cold buffer):</p> <ol> <li>Prepare 2 buffers. One is to buffer garbage collected data that is generally cold in cold buffer and the other is to buffer user updates(generally hot) in hot buffer</li> <li>Whenever the user updates or insert data, the data goes to the hot buffer. When the hot buffer is full, it flushes to a new block at the end of the log instead of the last unfull block.(In this system, every block will be full unlike the circular log in last section)</li> <li>Whenever the GC migrates cold data, it does not directly migrate to the last unfull block at the end of the log. It goes to the cold buffer. When the cold buffer is full, it flushes to a new block. Then in this case, hot block and cold block are separate from each other.</li> </ol> <p>write cost: O((1 + GC)/B)</p> <p>soln 3(more advanced, use data structure \"count-min\" to separate data with different temperatures):</p> <ol> <li>We prepare multiple buffers(more than 2). One is the really hot buffer that is used to buffer usr insert and the other are the buffers of different temperature  ranges indicated by count-min.</li> <li>Whenever we update key/value pair, we insert the blockId where the kv lies in to the count-min. Then we query the temperature of that block. We then put that kv into the buffer associated with that buffer</li> <li> <p>Whenever we perform GC, we query the temperature of that selected block and put it in the buffer associated with that temperature.</p> </li> <li> <p>count-min: a data structure that estimates freqency of elements in a data stream. Consists of d arrays of w counters. Each counter is an integer type.</p> </li> <li> <p>insert: when we insert an element, for each array of d arrays, we use a different hash function to get its position in each array and increment the counter at the hashed position in each array</p> </li> <li> <p>query: when we query an element's frequency, we hash it to each array and get the minimum counter of each array as the frequency.</p> </li> <li> <p>hyperparameter of count-min(w, d):</p> <ul> <li> <p>w = \\(\\lceil e / \\epsilon \\rceil\\)</p> </li> <li> <p>d = \\(\\lceil ln (1 / \\delta) \\rceil\\)</p> </li> </ul> <p></p> </li> <li> <p>decay(deal with situation past hot pages become cold at some time): every x insertion, we divide all counters by 2</p> </li> </ol> <p>write cost: O((1 + GC)/B)</p> <p>impact of hot/cold separation:</p> <p>let WA<sub>sep</sub> be the GC overhead employ hot/cold separation,</p> <p>then WA<sub>sep</sub> &lt;= 1 +\\(\\frac{L/P}{2 *(1 - L/P)}\\) &lt;= 1 +\\(\\frac{L/P}{(1 - L/P)}\\)</p>"},{"location":"csc443/#3-checkpoint-recovery","title":"3. Checkpoint recovery","text":"<p>Problem:</p> <p>When power is suddenly off, if we want to restore the data in index and live-bytes, we need to read all logs on storage from newest to oldest. However, this is very slow if we have many logs</p> <p>Attemp4(Simple recovery algorithm):</p> <p>based on Sol 1, 2, or 3</p> <ul> <li> <p>Recovery:</p> </li> <li> <p>scan the log from the most recent block to the oldest block</p> </li> <li> <p>For each block of the log:</p> <ol> <li> <p>add the block_id and the file size pair to the live-byte</p> </li> <li> <p>For each entry of the block:</p> <ol> <li>if the entry not in index:</li> <li> <p>add the entry's key and blockId pair to the index</p> </li> <li> <p>else:</p> </li> <li>subtract the entry's value size from the live-byte through blockId</li> </ol> </li> </ol> </li> </ul> <p>Problem:</p> <ul> <li>slow for large data</li> </ul> <p>Soln 5(Checkpointing):</p> <p>based on soln 1, 2, or 3</p> <ul> <li> <p>before crash:</p> </li> <li> <p>Every X updates/insertions, store copy of index &amp; live bytes.</p> </li> <li> <p>After crash/during recovery</p> </li> <li> <p>load all copies of index &amp; live-bytes</p> </li> <li> <p>scan X latestest block(Since the copy is the index before at most X insert/updates)</p> <p>For each block:</p> <ol> <li>For each entry in the block:<ol> <li>if the entry not in index:</li> <li>add the entry's key and blockId pair to the index</li> <li>else:</li> <li>subtract the entry's value size from the live-byte through blockId</li> </ol> </li> </ol> </li> </ul> <p>cost: </p> <ul> <li> <p>write: O(GC/B + index size / X)</p> </li> <li> <p>recovery: O(X/B) reads for backwards scan</p> </li> <li>get: O(1)</li> </ul> <p>Note:</p> <p>After we create the copy of index and insert/update M(M &lt; X) times, we may delete some key from index and the copy cannot reflect deleting in the old way. To achieve this, we add tombstone to the key  in buffer during delete operation and then remove from index. Then the log will reflect tombstone. When we scan the X logs during recovery, if the first instance of a key we see is a tombstone, we ignore all subsequent entries with this key.</p>"},{"location":"csc443/#4-cuckoo-filter","title":"4. Cuckoo filter","text":"<p>Why do we need Cuckoo filter:</p> <p>The problem is that index size is quite big, Index size= N * (P + K) / \\(\\alpha\\)</p> <ul> <li>N = data size</li> <li>K = each key size</li> <li>\\(\\alpha\\): collision resolution overheads(approximately 0.8)</li> </ul> <p>We can reduce K by adopting Cuckoo filter if we use Cuckoo filter to implement Index. The Cuckoo filter reduces space in memory by fixing an arbitrarily large,variable-length keys to be fixed size fingerprint</p> <p>Note:</p> <p>payload in slide: The payload can be any small fixed-length value you wish to associate with a fingerprint. In the case of circular logs, the payload is a pointer to the location of the entry in storage. </p> <p>HyperParameters of Cuckoo filter:</p> <ul> <li>M: bits length of each finger-print in Cuckoo filter bucket</li> <li>\\(\\alpha\\): fraction of occupied slots</li> <li>\\(\\beta\\): number of slots in bucket, the slide sets it to be 4</li> <li>N: the length of Cuckoo filter(i.e: number of buckets)</li> <li>h1(): primary hash function</li> <li>hash(): a function in alternate hash function h2(), h2() composites hash()</li> <li>h2(): h1(X) XOR hash(FP(X))</li> <li>FP(): finger-print function</li> </ul> <p>Invariant theory about Cuckoo filter:</p> <p>Soln 6(Implementation with Cuckoo filter, based on soln 5):</p> <ul> <li> <p>Initialize: initialize N buckets and each bucket can store \\(\\beta\\)(in slide, this is 4) pairs of . We prepare h1() function that maps key X to {1, 2, ... N} and h2() function to be <code>h1(X) XOR hash(FP(X))</code> <li> <p>insert(Y, V<sub>Y</sub>):</p> </li> <li> <p>add (Y, V<sub>Y</sub>) to the buffer/change the tombstone to V<sub>Y</sub></p> </li> <li> <p>If (the buffer is full):</p> <ol> <li> <p>flush all  to the disk location appointed by the buffer(may be temperature buffer) <li> <p>set the live-bytes data</p> </li> <li> <p>For each key <code>Y</code> that is not a tombstone in the buffer:</p> <ol> <li> <p><code>var flag = false</code></p> </li> <li> <p>if (primary bucket or alternative bucket contains FP that equals to <code>FP(Y)</code>):</p> </li> <li> <p>for each <code>FP</code> in primary bucket and alternative bucket that equals to <code>FP(Y)</code>:</p> <ol> <li>issue read to the <code>blockId</code></li> <li>if (key <code>Y</code> is in block pointed by <code>blockId</code>):         1. update <code>blockId</code> to point to the new <code>blockId</code>         2. set <code>flag</code> to true  // if flag is false, after this for loop, it means we have false positive</li> </ol> </li> <li> <p>if (!flag)    // false positive case or negative case, in total, key has never been inserted</p> </li> <li> <p>if (the bucket at<code>h1(FP(Y))</code> in index is empty): </p> <ol> <li>add  to the h1 bucket <li>else:<ol> <li>if (the bucket at alternative bucket of <code>FP(Y)</code> in index is empty)<ol> <li>add  to the alternative bucket <li>else:<ol> <li>evict a random fingerprint to its alternative bucket and do it recurisively</li> </ol> </li> <li> <p>delete(Y):</p> </li> <li> <p>set variable <code>flag</code> to be false</p> </li> <li> <p>if (primary bucket or alternative bucket contains FP that equals to <code>FP(Y)</code>):</p> <ol> <li> <p>for each <code>FP</code> in primary bucket and alternative bucket that equals to <code>FP(Y)</code>:</p> <ol> <li> <p>issue read to the blockId</p> </li> <li> <p>if (key <code>Y</code> is in the block pointed by <code>blockId</code>):</p> </li> <li> <p>delete that mapping &lt;<code>FP</code>, <code>blockId</code>&gt; from cuckoo filter</p> </li> <li> <p>set <code>flag</code> to be <code>true</code> // we have this key in cuckoo filter </p> </li> </ol> </li> </ol> </li> <li> <p>if (flag)   // this means we find the key in cuckoo filter, otherwise, we do nothing</p> <ol> <li>write &lt;<code>Y</code>, tombstone&gt; to the buffer</li> </ol> </li> <p>\u200b     </p> <ul> <li> <p>update:</p> </li> <li> <p>call delete()</p> </li> <li>call insert()</li> </ul> <p>cost:</p> updates/deletes O(1 + 2<sup>-M+3</sup>) read &amp; O(GC/B)writes insert O(2<sup>-M+3</sup>) read &amp; O(GC/B) writes Gets O(1 + 2<sup>-M+3</sup>)"},{"location":"csc443/#week-10","title":"Week 10","text":""},{"location":"csc443/#1-selection","title":"1. Selection","text":"<p>Context: </p> <p>In one single table, we want to execute <code>SELECT * FROM table A WHERE X=\"i\" and Y=\"j\"</code>. Let     |\\(X_i\\)|  and |\\(Y_j\\)| denote the number of matching rows to X = i and Y = j. Let total number of rows in table A be n</p> <p>Here is  how select query is implemented:</p> <ul> <li> <p>algorithm 1:</p> </li> <li> <p>We store 2 unclustered index for the 2 columns for one table. In this case, the index leaf node stores the pointer to the real data (O(log<sub>B</sub>(N)))</p> </li> <li>Let Y<sub>j</sub> be the set of rowId that points to the table row and matches Y = j. Search index Y(say pick index Y) for the start of the result set Y<sub>j</sub> and scan the index leaf nodes for all pointers that are in Y<sub>j</sub></li> <li>For each pointer in Y<sub>j</sub>, we read the actual row from the table and check if its x column is j(This takes O(|Y<sub>j</sub>|))</li> </ul> <p>cost: logB(N) + |Yj| I/O</p> <ul> <li>algorithm 2:</li> <li>We store 2 unclustered index for the 2 columns for one table</li> <li>Let X<sub>i</sub>, Y<sub>j</sub> be the result set of rowId. search both indices for the start pointerof X<sub>i</sub>, Y<sub>j</sub> (O(2 * log<sub>B</sub> N)) and scan the leaf nodes for both indexes for all pointers that are in X<sub>i</sub> and Y<sub>j</sub> (O(|X<sub>i</sub>|/ B + |Y<sub>j</sub>| / B))</li> <li>For each shared rowId in X<sub>i</sub> and Y<sub>j</sub>, we look into the table for the desired column in table O(|Xi\u2229Yj|)</li> </ul> <p>\u200b       Cost: 2 \u00b7 logB(N) + |Xi|/B + |Yj|/B + |Xi\u2229Yj| I/O</p> <ul> <li> <p>algorithm 3(composite index):</p> </li> <li> <p>We store an unclustered B-tree that has the concatnate string of X and Y as the key of the index(similar to the format \"x, y\" as the key)</p> </li> <li>We search the composite index for key \"i, j\" to find the start pointer(O(log<sub>B</sub>(N)))</li> <li>We then scan the leaf node for all (key, rowId) of the composite index and add the rowIds to the set. For each rowId in the set, we look into the row of the table and this in total costs |Xi\u2229Yj|</li> </ul> <p>cost: log<sub>B</sub>(N) + |X<sub>i</sub> \\(\\cap\\) Y<sub>j</sub>| </p> <p>downside: Cannot handle queries just based on Y</p>"},{"location":"csc443/#2-projection","title":"2. Projection","text":"<p>Select specific columns from the table</p>"},{"location":"csc443/#3-order-by","title":"3. Order by","text":"<ul> <li>index</li> <li>external sorting</li> </ul>"},{"location":"csc443/#31-index","title":"3.1 index","text":"<ul> <li>unclustered index: good for small input, but bad for large input because of random I/O</li> <li>clustered index: good for all cases</li> </ul>"},{"location":"csc443/#32-external-sort","title":"3.2 external sort","text":""},{"location":"csc443/#4-distinct","title":"4. Distinct","text":""},{"location":"csc443/#41-sort","title":"4.1 sort","text":"<ol> <li>Sort(through quick-sort, heap sort, external sort, index scan)</li> <li>eliminate adjacent identical items</li> </ol> <p>cost: O(Nlog<sub>2</sub>(N))</p>"},{"location":"csc443/#42-hash","title":"4.2 Hash","text":"<ol> <li>Prepare a hashtable with the item as the key and boolean value as the value</li> <li>For each item, query if the item is in the hashtable:</li> <li>if in, then this is a duplicate item</li> <li>if not in, then this is not a duplicate item and add the item to the hash table</li> </ol> <p>cost: O(N)</p>"},{"location":"csc443/#5-group-by","title":"5. Group by","text":""},{"location":"csc443/#51-what-is-group-byexample","title":"5.1 What is group by(example)","text":"<p>Suppose we have a select query as following. The group by groups the rows that has the same address and returns the sum of income for each address.</p> <p></p>"},{"location":"csc443/#52-implementation-1-sort","title":"5.2 Implementation 1: sort","text":"<ol> <li>sort the table rows by specific column</li> <li>group the rows with the same column value</li> </ol> <p>\u200b   cost: O(Nlog<sub>2</sub>N)</p>"},{"location":"csc443/#53-implementation-2-hash","title":"5.3 Implementation 2: hash","text":"<ol> <li>Prepare a hashtable with the specific column as the key and list of rows as the value</li> <li>For each row of the table, we hash the specific column value:</li> <li>if the column value is in the hash table, we append the row to this key/value pair</li> <li>if not, we add the column value to the table and append this row to the key's empty list</li> </ol>"},{"location":"csc443/#6-join","title":"6. Join","text":"<p>Join algorithm:</p> <ul> <li>nested loop</li> <li>block nested loop</li> <li>index-join</li> <li>sort-merge join</li> <li>grace hash join</li> </ul> <p>What is join(example query):</p> <pre><code>SELECT ... FROM T1, T2 WHERE T1.B = T2.B\n</code></pre> <p></p>"},{"location":"csc443/#61-nested-loop","title":"6.1 nested loop","text":"<p>Algorithm:</p> <ol> <li>For each entry in the join column of  table T1: </li> <li> <p>For each page of table T2:</p> <ol> <li>Scan and read the whole page of table T2</li> <li>find the rows that has the same join column as the entry in this page</li> <li>join the rows in table T2 with the row where the entry lies in table T1 and add to the result set</li> </ol> </li> <li> <p>cost(O(|T1| \u00b7 |T2|/B) I/O(can be improved by block nested loop)</p> </li> </ol>"},{"location":"csc443/#62-block-nested-loop","title":"6.2 block nested loop:","text":"<p>algorithm 1:</p> <ol> <li>For each page a in table T1:</li> <li>scan and read the whole page t1</li> <li>For each page t2 in table T2:<ol> <li>scan and read the whole page t2b</li> <li>join the rows in page t2 that has the same join column value as any of the rows in page t1</li> </ol> </li> </ol> <p>variation 1:</p> <ul> <li> <p>For each block in one relation, scan whole other relation</p> </li> <li> <p>Cost: O(|T1|/B \u00b7 |T2|/B) I/O</p> </li> </ul> <p>variation 2:</p> <ul> <li>we read Q pages from T1 for each scan of T2</li> <li>Cost: O(|T1|/(B \u00b7 Q) \u00b7 |T2|/B) I/O</li> </ul>"},{"location":"csc443/#63-index-join","title":"6.3 Index join","text":"<p>Algorithm1: (suppose we have only have 1 index on the joint column of a table):</p> <ol> <li>For each entry in one relation T1, </li> <li>search index for other relation T2</li> </ol> <p>cost: O(|T1| * log<sub>B</sub>(|T2|))</p> <p>Algorithm2: (Suppose we have 2 index on the joint column of a table, assume unclustered index):</p> <ol> <li> <p>Let 2 pointers <code>ptr1</code>, <code>ptr2</code> point to the first kv pair of the leftmost leaf node of the 2 index. </p> </li> <li> <p>traverse both index. </p> </li> </ol> <p>while(ptr1 &lt; number of total entries in index 1 &amp;&amp; ptr2 &lt; number of total entries in index 2):</p> <ol> <li>if <code>(*ptr1). key</code>&lt; <code>(*ptr2).key</code>:<ol> <li>ptr1++</li> </ol> </li> <li>else if <code>(*ptr1). key</code>&gt; <code>(*ptr2).key</code>:<ol> <li>ptr2++</li> </ol> </li> <li>else:<ol> <li>get the row pointer from T1 index entry and find the row t1 from T1 through this pointer</li> <li>get the row pointer from T2 index entry and find the row t2 from T2 through this pointer</li> <li>join row t1 and row t2</li> </ol> </li> </ol> <p>cost: O(|T1| + |T2|), since we need to look through the table from the pointer of the index in each iteration</p>"},{"location":"csc443/#64-sort-merge-join","title":"6.4 sort-merge join","text":"<ol> <li>Sort both relations based on join key</li> <li>Scan both relations linearly</li> </ol>"},{"location":"csc443/#65-grace-hash-join","title":"6.5 grace - hash join","text":"<p>algorithm: </p> <p>Let T1 be the smaller table, let T2 be the larger table</p> <ol> <li>Prepare G buffers for the output buffer. Prepare 1 buffer for the input buffer b0. Prepare hash function h1 that has range set {1, 2, ... G} and hash function h2.</li> </ol> <p>\u200b   phase 1(partition T1):</p> <ol> <li>Read each page of T1 into the input buffer b0</li> <li>For each entry of the input buffer page:</li> <li>apply h1 on the join column value of the entry and assign the entry to the corresponding output buffer index according to the hash value</li> <li>if this output buffer is full, flush to the file on disk and we can name this file as \"t1-{outputbuffer_index}\"</li> <li>Once we have done partitioning all the pages for T1. We check the G partition files' sizes on disk. If one partition's size is greater than the total size of buffer pages(G-2 buffer size), we will treat this partition as the T1 in step1 and repartition the partition by applying h1 on the join column value of entry in this file concatenated with \"{outputbuffer_index}\" until the partition size fits the memory</li> </ol> <p>\u200b   Phase 2(partition T2):</p> <ol> <li>Read each page of T2 into the input buffer page b0</li> <li>For each entry of the input buffer page:</li> <li>apply h1 on the join column value of the entry and assign the entry to the corresponding output buffer index according to the hash value. If this buffer index is repartioned in step1, we will later need to repartition this buffer again by applying h1 on the join column value concatenated with the outputBuffer index</li> <li>if this output buffer is full, flush to the file on disk and we can name this file as \"t2-{outputbuffer_index}\"</li> <li>Once we have done partition all the pages for T1. We don't care if the partitions of t2 fit in memory. However, if this partition group index is repartitioned in stage 1, we need to repartition again</li> </ol> <p>Phase 3(Join):</p> <ol> <li>Build the hash table with G-2 pages</li> <li>Apply h2 on each entry of T1's partition file and load the T1's partition file into the hashtable(T1's partition file is guranteed to be in memory)</li> <li>we read each page of T2's partition into a separate input buffer and apply h2 on each entry of the page to see if this entry's hash value in hash table. If it matches, we delete the entry from the hashtable and output to the output buffer. If the output buffer is full, we flush it to the join table</li> </ol> <p>cost(2-pass partition for phase 1 &amp; 2, no need to repartition):</p> <ul> <li>i/o: O(|T1|/B + |T2|/B)</li> <li>CPU: O(|T1| + |T2|)</li> <li>Memory: O(\\(\\sqrt {min(|T1|, |T2|) * B}\\))</li> </ul>"},{"location":"csc443/#7-cardinality-estimation","title":"7. Cardinality Estimation","text":"<p>What is cardinanlity estimation:</p> <p>In selection query, we have algorithm 1, 2 and 3. There cost are the following:</p> <ul> <li>log<sub>B</sub>(N) + |Y<sub>j</sub>| I/O</li> <li>log<sub>B</sub>(N) + |X<sub>i</sub>|I/O</li> <li>2 * log<sub>B</sub>(N) + |X<sub>i</sub>|/B + |Y<sub>j</sub>|/B + |X<sub>i</sub> \\(\\cap\\) Y<sub>j</sub>| I/O</li> </ul> <p>They all depend on |Y<sub>j</sub>|, |X<sub>i</sub>| and |X<sub>i</sub> \\(\\cap\\) Y<sub>j</sub>|. In order to select the best algorithm, we need to know these terms in advance. The way we can do that is to estimate these terms. The approximation is |X<sub>i</sub>| \\(\\approx\\) N / |X|. Therefore, we need to further estimate |X|. The cardinality esimation is to estimate number of unique values of a column</p>"},{"location":"csc443/#71-add-to-the-index","title":"7.1 Add to the index","text":"<p>let X<sub>i</sub> be the set of rows where X = i,</p> <p>if we have an index on column X, we maintain a cardinality counter |X|(where |X| refers to the unique number of X values in the table)</p> <p>then |X<sub>i</sub>| \\(\\approx\\) N / |X|</p>"},{"location":"csc443/#72-k-minimum-valuekmv-sketch","title":"7.2 K Minimum Value(KMV) sketch","text":"<p>algorithm:</p> <ul> <li> <p>Initialize sketch:</p> </li> <li> <p>We prepare a hash function that takes the X column value and outputs a hash number and a set of size K that can contain K hash number. We call this set a sketch.</p> </li> <li> <p>We first use the hash function to hash each column value of column X. We select K minimum hash number from these column values and put in the sketch</p> </li> <li> <p>Insert column value:</p> </li> <li>Whenever we insert a row to the table, we compute this row's column hash value and compares the hash value with the K hash value in the sketch, if this hash value is smaller than any of the K hash values in the sketch, we pop the max hash value in the sketch and push this hash value into the sketch</li> <li>Cardinality estimimation:</li> <li>|X| = (K - 1) * |hashSpace size| / (maximum hash value in sketch)</li> </ul>"},{"location":"csc443/#73-histogram","title":"7.3 Histogram","text":"<p>Algorithm:</p> <ol> <li>Use histogram to get statistic on number of counts of elements in range of buckets</li> </ol> <p></p> <ol> <li>Estimate |X<sub>i</sub>| = (bucket counts) / bucket range</li> </ol>"},{"location":"csc443/#74-countmin","title":"7.4 CountMin","text":""},{"location":"csc443/#8-query-optimization","title":"8. Query optimization","text":"<p>We can optimize the query through the following methods or principles:</p> <ul> <li>Pushing Selections &amp; Projections</li> <li>Operator Fusion</li> <li>Consciousness of next operator</li> <li>Restrict join space</li> </ul>"},{"location":"csc443/#81-pushing-selectionsprojections","title":"8.1 Pushing Selections&amp;Projections","text":"<p>What is pushing selections&amp;projection:</p> <p>We can project out useless columns if this column is no longer used after we have made selection based on this column. The goal is to reduce data size flowing through subsequent operators</p> <p>Ex:</p> <p></p> <p>if we want to execute query as following on the above T1 and T2:</p> <pre><code>Select A, C from T1, T2 where T1.B = T2.B and A=\u201c\u2026\u201d and D=\u201c\u2026\u201d \n</code></pre> <p>Unoptimized flow:</p> <ol> <li>selection on T1 and T2:</li> <li>select rows based on A in T1</li> <li>select rows based on D in T2</li> <li>join the selected rows from T1 and T2 through column B(the result table has column: A, B, C, D)</li> <li>project out column B and D in the table </li> </ol> <p>Optimized flow:</p> <ol> <li>selection on T1 and T2:</li> <li>select rows based on A in T1</li> <li>select rows based on D in T2 and project out column D</li> <li>join the selected rows from T1 and T2 through column B(the result table has column: A, B, C)</li> <li>Project out column B in the table from step 2</li> </ol>"},{"location":"csc443/#82-operator-fusion","title":"8.2 Operator fusion","text":"<p>What is operator fusion:</p> <p>Sometimes we can merge work from across multiple logical operators(such as selection, projection) into one operator</p> <p>Example:</p> <p>the grace hash join can fuse the selection and projection in step1 into partition pass phase of join operation and fuse step3 into the join pass of join operation</p>"},{"location":"csc443/#83-consciousness-of-next-operation","title":"8.3 Consciousness of next operation","text":"<p>What is:</p> <p>Choosing the algorithm for given operator now may cheapen subsequent ones</p> <p>Example:</p> <p>if we want to execute:</p> <pre><code>Select * from T1, T2 where T1.X = T2.X Order by X\n</code></pre> <p>The unoptimized flow:</p> <ol> <li>join through join hash algorithm</li> <li>sort by</li> </ol> <p>The optimized flow:</p> <ol> <li>join through sort-merge algorithm</li> </ol>"},{"location":"csc443/#84-restrict-join-operation","title":"8.4 Restrict join operation","text":"<p>We can execute a complex join in many different orders.</p> <p>Optimizers only consider left-deep joins to restrict the search space</p>"},{"location":"csc443/#week-11","title":"Week 11","text":""},{"location":"csc443/#1-row-store-vs-column-store","title":"1. Row store vs column store","text":"<p>Example(Row-store): Postgres, MariaDB, etc</p> <p>Example(Column-store): MonetDB, Vectorwise, C-Store, Vertica</p> <p>Oracle, IBM, Microsoft were Row-store, but now they offered both</p> <p>advantage: </p> <ul> <li>fast if we want to query few columns</li> </ul> <p>disadvantage:</p> <ul> <li>bad performance when we insert/update</li> </ul>"},{"location":"csc443/#2-data-storage-alignment","title":"2. Data storage alignment","text":"<p>2.1 How to idenify two different column values belong to the same row</p> <ul> <li>Implementation attempt:</li> </ul> <p>For each column, we store a materialized ID</p> <p></p> <ul> <li> <p>Drawback: </p> </li> <li> <p>waste storage space because we are storing a new column ID for each column</p> </li> <li> <p>slow down queries by reading more data</p> </li> <li> <p>Soln(positional alignment):</p> </li> </ul> <p>We make the data from different columns in the same offset belong to the same row in column store.</p> <p>Then,</p> <p>\u200b Col1(i) = i * width(col1)</p> <p>\u200b Col2(i) = i * width(col2)</p>"},{"location":"csc443/#3-get-query","title":"3.  get query","text":"<p>Suppose we have following queries:</p> <p></p> <p>Attempt to implement 1(early materialization):</p> <p>algorithm:</p> <ol> <li>scan column A. </li> </ol> <p>For each entry we find in column A that satisfies predicate(A):</p> <ol> <li>calculate the entry offset <code>i</code> in A and read colB(i)</li> <li>if colB(i) satisfies predicate(B):<ol> <li>we read colC(i)</li> </ol> </li> </ol> <p>Problem: this involves random I/O. </p> <ul> <li>We need to jump from column A to column B to column C and jump back. This is random I/O</li> </ul> <p>soln 2(late materialization: process one column by a time):</p> <ul> <li> <p>algorithm:</p> </li> <li> <p>scan column A</p> </li> </ul> <p>For each entry we find in column A that satisfies predicate(A)</p> <ol> <li> <p>put its offset in the bitmap <code>a</code></p> </li> <li> <p>For each offset <code>ia</code> in bitmap <code>a</code>:</p> </li> <li> <p>check colB(ia) and see if it satisfies predicate(B)</p> </li> <li> <p>if it satisfies: append to the bitmap <code>b</code></p> </li> <li> <p>For each offset <code>ib</code> in bitmap <code>b</code>:</p> </li> <li> <p>read colC(ib)</p> </li> <li> <p>Advantage:</p> </li> <li> <p>Comparing with early materialization, we are sequentially reading a column</p> </li> <li> <p>how to determine the order of column to query:</p> </li> <li>KMV sketch</li> <li>histograms</li> <li>count-min</li> </ol>"},{"location":"csc443/#4-scan","title":"4. scan","text":"<p>To speed up scan,</p> <ul> <li>we add zone maps, we extra store min-max for each Equally sized partitions (partition size: MBs or GBs).</li> </ul> <p>Scan:</p> <ol> <li>access each partition of data.</li> </ol> <p>For each partition of data:</p> <ol> <li> <p>check if the relevant range is within min and max; if not, skip to the next partition</p> </li> <li> <p>if so, access and read in each page of the partition</p> <p>For each page of the partition:</p> <ol> <li>check if each entry of the page satisfies condition and add to the result set if so.</li> </ol> </li> </ol>"},{"location":"csc443/#5-insert","title":"5. Insert","text":"<ul> <li>attempt 1(In-place updates):</li> </ul> <p>algorithm:</p> <ol> <li>Directly add to the end of each column</li> </ol> <p>cost: O(#cols) I/O</p> <ul> <li>soln 2(In-memory buffering):</li> </ul> <p>algorithm:</p> <ol> <li>Prepare a buffer</li> <li>Whenever we insert multiple columns, we align them as rows in buffers</li> <li>If the buffer is full, we flush them and append to the end of each column</li> </ol>"},{"location":"csc443/#6-delete","title":"6. Delete","text":"<ul> <li>soln</li> </ul> <p>algorithm:</p> <ol> <li>use a buffer</li> <li>Whenever we delete, we add the offset of the deleted column to the buffer. When we query, we also need to check this buffer.If the buffer is full, we rewrite the whole column without including the column at offset in buffer</li> </ol> <p>O(N / (Buffer size \u00b7 B))</p>"},{"location":"csc443/#7-update","title":"7. Update","text":"<ul> <li>soln</li> </ul> <p>algorithm:</p> <ol> <li>Use 2 buffers</li> <li>The first buffer stores the deleted offset and associated timestamp. The second buffer stores the </li> </ol>"},{"location":"csc443/#8-simd","title":"8. SIMD","text":"<p>What is SIMD instruction:</p> <p>Apply one instruction in parallel to multiple values within one cache line(e.g: 128 bits-256 bits at a time). This instruction is used to accelate get query and so on</p> <p>Ex from slide:</p> <p>Suppose we have query <code>select sum(A) from table where A &gt; 5</code> and the column A is in cache with value(3, 1, 7, 4, 8, 9, 2,8), here is what SIMD does:</p> <ol> <li>perform check with (A &gt; 5) on every value to obtain boolean value</li> </ol> <p></p> <ol> <li>multiply the value by the boolean value in cache through SIMD</li> </ol> <p></p> <ol> <li>load the resulting value in cache line from step 2 to cpu registers/counters(where the counter value is initially 0 in the following screen) through SIMD</li> </ol> <p></p> <ol> <li>Sum the C1 to C8 register values together to get the result</li> </ol>"},{"location":"csc443/#9-compression","title":"9. Compression","text":"<p>(slide p99 - )</p> <p>Why do we need compression:</p> <p>The goal is not only to save storage space, but also reduce I/O if we save storage space to improve performance</p> <p>Here r 3 ways to compress:</p>"},{"location":"csc443/#91-bit-vector-encoding","title":"9.1 Bit vector encoding","text":"<p>(slide p 105)</p> <p>Encode one bit string or one bit for each possible value indicating if the entry has the given value </p> <ul> <li>Pro: good and fast if we don't have many unique values for this column</li> <li>con: Only applicable if there are very few values</li> </ul>"},{"location":"csc443/#92-dictionary-encoding","title":"9.2 Dictionary encoding","text":"<p>(slide p109)</p> <ol> <li>employ a dictionary with smaller strings to represent the original column value</li> <li>replace the original column value with the smaller string in the compressed column</li> </ol>"},{"location":"csc443/#93-run-length-encoding","title":"9.3 Run-length encoding","text":"<ol> <li> <p>represent repeating values using one entry</p> </li> <li> <p>pro: compatible with dictionary encoding and can further improve compression</p> </li> <li>cons: must scan column to get entry at a given entry. In the original column, if we are given the offset <code>i</code>, we can just find col(i) in O(1) time. However, in the compressed column, the i<sup>th</sup> value is compressed, we need to determine which value's run the col(i) belongs to.</li> </ol>"},{"location":"csc443/#10-index-get-query","title":"10. Index get query","text":"<ul> <li>sort late materialization</li> <li>column projections</li> <li>database cracking</li> </ul> <p>Suppose we have an unclustered index and a table of column A, B, C, D. We want to execute the following query:</p> <pre><code>Select avg(B) where A &gt; 5 and A &lt; 10\n</code></pre> <p>Algorithm 1(combining with late materialization, ex from slide p127):</p> <ol> <li>we search the index for the start of the query element</li> <li>we scan the index from the start and add every quelified key's associated disk pointer to the intermediate set</li> <li>we sort the disk pointer(which is the offset)</li> <li>Then we search column B from the sorted disk pointer(Access to column B becomes \u201cskipsequential\u201d rather than random from step 3)</li> </ol> <p>Algorithm 2(Column projection):</p> <ul> <li>Initialized data: we have the column A be sorted. We also duplicate column B be sorted by A so that each column offset from B matches each column offset from A.</li> <li>Query &amp; scan:</li> <li>We scan the column A to find the matching column ID from A</li> <li>Since A is sorted and B is sorted by A, the column ID will automatically sorted. When we look column B through the column IDs, the lookup becomes skip sequential rather than random</li> </ul> <p>Algorithm 3(db cracking):</p> <ul> <li>Initialize:</li> <li>create a cracking column copy of the column</li> <li>we also duplicate other columns to be ordered by this column</li> <li>Query &amp; scan:</li> <li>When we query/scan based on some upperbound/lowerbound, we will use this bound as pivot to partition the cracking column</li> </ul>"},{"location":"csc443/#week-12","title":"Week 12","text":""},{"location":"csc443/#1-transaction","title":"1. Transaction","text":"<p>transaction component:</p> <ul> <li>begin txn</li> <li>some commands in SQL</li> <li>Commit or abort</li> </ul>"},{"location":"csc443/#2-common-inconsistency-problem","title":"2. Common inconsistency problem","text":"<p>ACID prevents the following inconsistency problems:</p> <ul> <li>system failure</li> <li>power failure</li> <li>hardware failure</li> <li>data center failure</li> <li>concurrency:</li> <li>dirty read</li> <li>Unrepeatable read</li> <li>phantom read: during a transaction, new rows are added (or deleted) by another transaction to the records being read between 2 accesses of the transaction</li> </ul>"},{"location":"csc443/#3-lock","title":"3. Lock","text":"<ul> <li>simple soln: lock</li> </ul>"},{"location":"csc443/#31-shared-read-lock","title":"3.1 Shared read lock","text":"<p>Shared locks allow multiple transactions to read the same data, but prevent any transaction from modifying it</p> <p>Dirty read: a transaction reads a modified but uncommited data item from other transactions</p>"},{"location":"csc443/#32-exclusive-lock","title":"3.2 Exclusive lock","text":"<p>Exclusive locks allow a transaction to modify the data, but prevent any other transaction from reading or modifying it</p> <p>unrepeatable read anomaly: subsequent reads of the same data are inconsistent as the data was changed in-between(these data are comitted)</p>"},{"location":"csc443/#33-compare-2-locks","title":"3.3 Compare 2 locks","text":"Shared read locks exclusive lock problem unrepeatable read dirty reads Issued when transaction wants to read item that do not have an exclusive lock. Issued when transaction wants to update unlocked item. Any number of transaction can hold shared lock on an item. Exclusive lock can be hold by only one transaction. Example: Multiple transactions reading the same data Example: Transaction updating a table row"},{"location":"csc443/#34-lock-manager","title":"3.4 Lock Manager","text":"<p>lock manager -  a hash table</p> <p></p> <ul> <li>type refers to: either shared read lock or exclusive write lock</li> <li>lock count: how many txns hold the shared lock</li> <li>queue of waiting requests: what txn to invoke next when this lock is released</li> </ul> <p>lock vs latches:</p> <p></p>"},{"location":"csc443/#4-inconsistency-problem-soln","title":"4.  Inconsistency problem soln","text":"<p>2 phase locking:</p> <ul> <li>in phase 1: lock a data item(e.g: row) when it is accessed first time(Invariant: a data item that has been accessed cannot be modified until txn commits)</li> <li>in phase 2: release all locks(commit)</li> </ul> <p>How lock is used on index query(Lock-Coupling):</p> <p>During the following query:</p> <pre><code>SELECT * FROM ...  WHERE ...\n</code></pre> <p>index locking:</p> <p>Instead of locking the whole table, we lock the B-tree leaf node where the the relevant range lies in.</p> <p>coupling algorithm(bTree selection):</p> <ol> <li>var parent = bTree.root</li> <li>lock the parent with shared read lock</li> <li>var curr = parent.child  // let curr points to the child of the parent on search path</li> <li>while(curr is not a leaf)</li> <li>lock the node pointed by curr with read lock</li> <li>if the curr node is successfully locked:<ol> <li>release the lock on parent</li> <li>parent = curr</li> <li>curr = parent.child // let curr points to the child of the parent on search path</li> </ol> </li> <li>lock curr node with shared lock</li> </ol> <p>How lock is used on update query:</p> <pre><code>update table set C = \u2018\u2026\u2019 where A \u2018\u2026\u2019\n</code></pre> <p>coupling algorithm(bTree update/insert):</p> <ol> <li> <p>var parent = bTree.root</p> </li> <li> <p>lock the parent with shared read lock</p> </li> </ol> <p>phase 1(traversal of bTree to find the leaf):</p> <ol> <li> <p>var curr = parent.child  // let curr points to the child of the parent on search path</p> </li> <li> <p>while(curr is not a leaf)</p> </li> <li> <p>lock the node pointed by curr with read lock</p> </li> <li>if the curr node is successfully locked:<ol> <li>if curr is not full:<ol> <li>release the lock on parent</li> </ol> </li> <li>parent = curr</li> <li>curr = parent.child // let curr points to the child of the parent on search path</li> </ol> </li> </ol> <p>phase 2(lock and update the leaf node):</p> <ol> <li> <p>lock <code>curr</code> node with exclusive write lock  // now the leaf node is locked by write lock</p> </li> <li> <p>if curr is not full:</p> </li> <li> <p>release the read lock on parent</p> </li> <li> <p>update the leaf node pointed by <code>curr</code></p> </li> <li> <p>else:  // insertion</p> </li> <li> <p>split the leaf node pointer by <code>curr</code></p> </li> <li>lock the newly splited node with exclusive lock</li> <li>upgrade the lock on node pointed by <code>parent</code> to write lock</li> <li>write the pointer to the newly splitted node in <code>parent</code> node</li> </ol> <p>phase 3(propagate split upwards):</p> <ol> <li> <p>curr = parent</p> </li> <li> <p>parent = parent.parent</p> </li> <li> <p>while(curr != bTree.root):</p> <ol> <li>if curr is full:</li> <li>split <code>curr</code> node</li> <li>lock the newly splitted node with exclusive lock</li> <li>upgrade the lock on node pointed by <code>parent</code> to write lock</li> <li>write the pointer to the newly splitted node in <code>parent</code> node</li> <li>curr = parent</li> <li>parent = parent.parent</li> </ol> </li> <li> <p>if curr is full:</p> <ol> <li>split <code>curr</code> node</li> <li>lock the newly splitted node with exclusive lock</li> <li>create a new root node and lock it with exclusive lock</li> <li>write the pointer to the newly splitted node in <code>parent</code> node</li> </ol> </li> <li> <p>release all locks(commit)</p> </li> </ol>"},{"location":"csc443/#5-deadlock-in-2-phase-locking","title":"5. Deadlock in 2 phase locking","text":"<p>how to prevent:</p> <p>SOln1:</p> <ul> <li>record before-image for all changes to the DB in a sequential log</li> <li>If transaction aborts before completing, we undo its changes via its before-images in the log</li> </ul> <p>Soln2(Timeout):</p> <p>Soln3(Abort on wait):</p>"},{"location":"csc443/#6-recovery","title":"6. Recovery","text":"<p>Background:</p> <p>We have the following system failure:</p> <ul> <li>Media failure(covered by RAID)</li> <li>Power failure</li> <li>Data center failure</li> </ul> <p>For this section, we cover how to recover from power failure</p> <p>What should be done if power failure occurs between a set of operations:</p> <ol> <li>wrap the set of operations  in a transaction</li> <li>write changes to by each transaction in a log</li> <li>after power fails, scan log and cancel effects of uncommited transactions</li> </ol> <p>For step 2, we have 3 different types:</p> <ul> <li>undo</li> <li>redo</li> <li>redo/redo</li> </ul>"},{"location":"csc443/#61-undo","title":"6.1 Undo","text":"<p>background</p> <p>When we want to update data from db,</p> <p>attempt 1:</p> <ul> <li> <p>Before failure:</p> </li> <li> <p>Load the original data from db to buffer</p> </li> <li>mark the transaction start in log buffer</li> <li>update the data to new data in buffer pool</li> <li>write the old data into the log buffer after transaction start</li> <li>flush the log buffer to log</li> <li>flush the new data from the buffer to the db</li> <li>add the commit tag of the txn in log buffer</li> <li> <p>flush the commit tag of all transactions(including this one and other) on log on disk</p> </li> <li> <p>During recovery:</p> </li> <li> <p>traverse the log backwards. </p> <p>For each of the uncommited transaction:</p> <ol> <li>read the data in transaction from the db to the buffer pool</li> <li>Set the data back to the old version recorded in transaction in buffer pool</li> <li>flush to the database</li> <li>add unroll record in transaction log</li> </ol> </li> </ul> <p>Problem:</p> <p>If we have a transaction that has not been commited at the start of the transaction log, we need to read till the begining of the transaction log. That would take long time.</p> <p>soln 2:(add checkpoint)</p> <ul> <li>checkpointing:</li> <li>When there is  some number of txn commit tags in log buffer, stop accepting new txns &amp; wait until all txn in log buffer commits to log</li> <li>flush to storage</li> <li> <p>add checkpoint record to log buffer and flush again</p> </li> <li> <p>Recovery:</p> </li> <li> <p>traverse from the end of the log up to first checkpoint record we encounter and scan each uncommited txns.</p> <p>For each of the uncommited transaction:</p> <ol> <li>read the data in transaction from the db to the buffer pool</li> <li>Set the data back to the old version recorded in transaction in buffer pool</li> <li>flush to the database</li> <li>add unroll record in transaction log</li> </ol> </li> </ul> <p>Undo problem:</p> <ul> <li>when we force the data from buffer pool to db, this entails lots of random I/O</li> </ul>"},{"location":"csc443/#62-redo","title":"6.2 Redo","text":"<ul> <li> <p>before failure:</p> </li> <li> <p>load the original data from db to buffer pool</p> </li> <li>mark the txn start in log buffer</li> <li>update the buffer pool and add new data of the transaction in log buffer</li> <li>mark commit in log buffer</li> <li>flush log buffer to disk</li> <li> <p>flush changed data at buffer to storage at leisure/buffer is full</p> </li> <li> <p>recovery:</p> </li> <li> <p>traverse log forward. </p> <p>For each transaction:</p> <ol> <li>if the transaction is not marked commited:<ol> <li>we mark the transaction as rollback and do nothing</li> </ol> </li> <li>else:<ol> <li>replay the txn</li> </ol> </li> </ol> </li> </ul> <p>problem:</p> <ul> <li>the buffer pool needs to hold the data until the transaction has comitted, while the undo method has already flushed the changed data from buffer after the txn from log buffer flushed and  before log buffer flush commit to disk</li> </ul>"},{"location":"csc443/#63-undoredo","title":"6.3 Undo/Redo","text":"<ol> <li>load the old data into buffer pool</li> <li>mark the txn start in log buffer</li> <li>for each update in transaction:</li> <li>update data in buffer and write its old version and new version in log buffer</li> <li>may flush to the log if leisure or not full</li> <li>the buffer pool may evict this data to db if leisure(but if the log has not been flushed, cannot flush even leisure)</li> <li>flush commited to transaction log</li> </ol> <p>recovery:</p> <ol> <li>scan the log up to the first checkpoint:</li> <li>for each txn:</li> <li>if the txn is uncommited(no commit tag in log):<ol> <li>undo and add rollback tag to the txn</li> </ol> </li> <li>else:<ol> <li>replay</li> </ol> </li> </ol>"}]}